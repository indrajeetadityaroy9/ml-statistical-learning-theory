{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Splines Tutorial - Foundations\n",
    "\n",
    "This notebook provides a comprehensive introduction to spline methods, covering:\n",
    "- Mathematical foundations and basis functions\n",
    "- Regression splines with manual knot placement\n",
    "- Natural cubic splines with boundary constraints\n",
    "- Smoothing splines with automatic regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from splines import (\n",
    "    truncated_power, \n",
    "    truncated_power_basis_matrix,\n",
    "    RegressionSpline,\n",
    "    NaturalCubicSpline,\n",
    "    SmoothingSpline\n",
    ")\n",
    "from utils import (\n",
    "    generate_sinusoidal_data, \n",
    "    generate_polynomial_data,\n",
    "    plot_spline_fit, \n",
    "    plot_basis_functions,\n",
    "    plot_cv_curve,\n",
    "    mean_squared_error, \n",
    "    r_squared\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Truncated Power Basis Functions\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "For a k-th order spline with knots at $t_1 < \\dots < t_m$, the truncated power basis consists of $(m+k+1)$ functions:\n",
    "\n",
    "- Polynomial terms: $g_1(x) = 1, g_2(x) = x, \\dots, g_{k+1}(x) = x^k$\n",
    "- Truncated power terms: $g_{k+1+j}(x) = (x - t_j)_+^k$, for $j = 1, \\dots, m$\n",
    "\n",
    "where $(x)_+ = \\max(x, 0)$ is the positive part function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 3, 500)\n",
    "knot = 0.5\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "degrees = [1, 2, 3]\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    y = truncated_power(x, knot, degree)\n",
    "    ax.plot(x, y, linewidth=2)\n",
    "    ax.axvline(knot, color='r', linestyle='--', alpha=0.5, label=f'knot at {knot}')\n",
    "    ax.set_title(f'$(x - {knot})_+^{degree}$', fontsize=14)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Spline Basis Functions\n",
    "\n",
    "For **cubic splines** (degree 3) with 3 interior knots, we have $3 + 3 + 1 = 7$ basis functions:\n",
    "- $g_1(x) = 1$\n",
    "- $g_2(x) = x$\n",
    "- $g_3(x) = x^2$\n",
    "- $g_4(x) = x^3$\n",
    "- $g_5(x) = (x - t_1)_+^3$\n",
    "- $g_6(x) = (x - t_2)_+^3$\n",
    "- $g_7(x) = (x - t_3)_+^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 500)\n",
    "knots = np.array([3.0, 5.0, 7.0])\n",
    "degree = 3\n",
    "\n",
    "basis_matrix = truncated_power_basis_matrix(x, knots, degree)\n",
    "\n",
    "print(f\"Basis matrix shape: {basis_matrix.shape}\")\n",
    "print(f\"Number of basis functions: {basis_matrix.shape[1]}\")\n",
    "print(f\"Expected: m + k + 1 = {len(knots)} + {degree} + 1 = {len(knots) + degree + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_basis_functions(x, basis_matrix, knots, \n",
    "                           title=\"Cubic Spline Basis Functions (Truncated Power Basis)\",\n",
    "                           max_functions=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Terms vs Truncated Terms\n",
    "\n",
    "Let's separate and visualize the polynomial terms and truncated power terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for i in range(degree + 1):\n",
    "    ax.plot(x, basis_matrix[:, i], label=f'$x^{i}$', linewidth=2)\n",
    "ax.set_title('Polynomial Terms', fontsize=14)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Basis value')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "for j, knot in enumerate(knots):\n",
    "    ax.plot(x, basis_matrix[:, degree + 1 + j], label=f'$(x - {knot})_+^3$', linewidth=2)\n",
    "    ax.axvline(knot, color='red', linestyle='--', alpha=0.3)\n",
    "ax.set_title('Truncated Power Terms', fontsize=14)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Basis value')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Polynomial Degree\n",
    "\n",
    "Compare basis functions for different polynomial degrees (linear, quadratic, cubic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 500)\n",
    "knots = np.array([3.0, 5.0, 7.0])\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    basis_matrix = truncated_power_basis_matrix(x, knots, degree)\n",
    "    \n",
    "    for i in range(basis_matrix.shape[1]):\n",
    "        ax.plot(x, basis_matrix[:, i], alpha=0.7)\n",
    "    \n",
    "    for knot in knots:\n",
    "        ax.axvline(knot, color='r', linestyle='--', alpha=0.2)\n",
    "    \n",
    "    ax.set_title(f'Degree {degree} Spline Basis\\n({len(knots) + degree + 1} functions)', fontsize=12)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('Basis value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Spline from Basis Functions\n",
    "\n",
    "Any spline can be written as a linear combination of basis functions:\n",
    "$$f(x) = \\sum_{j=1}^{m+k+1} \\beta_j g_j(x)$$\n",
    "\n",
    "Let's create a spline by choosing coefficients manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 500)\n",
    "knots = np.array([3.0, 5.0, 7.0])\n",
    "degree = 3\n",
    "\n",
    "G = truncated_power_basis_matrix(x, knots, degree)\n",
    "\n",
    "beta = np.array([1.0, 0.5, -0.1, 0.01, 0.2, -0.3, 0.15])\n",
    "\n",
    "f_x = G @ beta\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x, f_x, linewidth=3, label='Spline $f(x) = \\\\sum \\\\\\\\beta_j g_j(x)$')\n",
    "\n",
    "for knot in knots:\n",
    "    ax.axvline(knot, color='r', linestyle='--', alpha=0.3, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "ax.set_title('Spline as Linear Combination of Basis Functions', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficients β:\")\n",
    "for i, b in enumerate(beta):\n",
    "    print(f\"  β_{i+1} = {b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression Splines\n",
    "\n",
    "Given samples $(x_i, y_i)$ for $i=1,\\dots,n$, we estimate the regression function $r(x) = E(Y|X=x)$ by fitting a $k$-th order spline with knots at prespecified locations $t_1, \\dots, t_m$.\n",
    "\n",
    "We minimize:\n",
    "$$\\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^{m+k+1} \\beta_j g_j(x_i)\\right)^2 = \\|y - G\\beta\\|_2^2$$\n",
    "\n",
    "Solution: $\\hat{\\beta} = (G^T G)^{-1} G^T y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Cubic Regression Spline to Sinusoidal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.2, \n",
    "                                            x_range=(0, 10), frequency=1.0)\n",
    "\n",
    "knots = np.array([2.0, 4.0, 5.0, 6.0, 8.0])\n",
    "\n",
    "model = RegressionSpline(degree=3)\n",
    "model.fit(x_train, y_train, knots)\n",
    "\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_true_func = lambda x: np.sin(2 * np.pi * x / 10)\n",
    "\n",
    "fig = plot_spline_fit(x_train, y_train, x_test, y_pred, knots, \n",
    "                      y_true_func=y_true_func,\n",
    "                      title=\"Cubic Regression Spline: Sinusoidal Data\")\n",
    "plt.show()\n",
    "\n",
    "y_pred_train = model.predict(x_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r_squared(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Training MSE: {mse:.4f}\")\n",
    "print(f\"Training R²: {r2:.4f}\")\n",
    "print(f\"Number of knots: {len(knots)}\")\n",
    "print(f\"Number of parameters: {len(knots) + 3 + 1} = {len(model.coefficients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Number of Knots\n",
    "\n",
    "More knots → more flexibility → better fit (but risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.2, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "knot_configs = [\n",
    "    (np.array([5.0]), \"1 knot\"),\n",
    "    (np.linspace(2, 8, 3), \"3 knots\"),\n",
    "    (np.linspace(2, 8, 5), \"5 knots\"),\n",
    "    (np.linspace(1, 9, 10), \"10 knots\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (knots, label) in zip(axes, knot_configs):\n",
    "    model = RegressionSpline(degree=3)\n",
    "    model.fit(x_train, y_train, knots)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    y_pred_train = model.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, y_pred_train)\n",
    "    r2 = r_squared(y_train, y_pred_train)\n",
    "    \n",
    "    ax.scatter(x_train, y_train, alpha=0.5, s=30, label='Data', color='gray')\n",
    "    ax.plot(x_test, y_pred, 'b-', linewidth=2, label='Spline')\n",
    "    ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g--', alpha=0.5, label='True')\n",
    "    \n",
    "    for knot in knots:\n",
    "        ax.axvline(knot, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax.set_title(f'{label}\\nMSE={mse:.4f}, R²={r2:.4f}', fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Polynomial Degree\n",
    "\n",
    "Compare linear (degree=1), quadratic (degree=2), and cubic (degree=3) splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train, y_train = generate_polynomial_data(n_samples=50, degree=3, noise_std=0.3)\n",
    "x_test = np.linspace(-1, 1, 500)\n",
    "\n",
    "knots = np.linspace(-0.6, 0.6, 4)\n",
    "\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    model = RegressionSpline(degree=degree)\n",
    "    model.fit(x_train, y_train, knots)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    y_pred_train = model.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, y_pred_train)\n",
    "    \n",
    "    ax.scatter(x_train, y_train, alpha=0.5, s=30, color='gray')\n",
    "    ax.plot(x_test, y_pred, 'b-', linewidth=2)\n",
    "    \n",
    "    for knot in knots:\n",
    "        ax.axvline(knot, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    degree_names = {1: 'Linear', 2: 'Quadratic', 3: 'Cubic'}\n",
    "    ax.set_title(f'{degree_names[degree]} Spline (k={degree})\\nMSE={mse:.4f}', fontsize=12)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary Behavior Problem\n",
    "\n",
    "One problem with regression splines is that the estimates tend to display erratic behavior, i.e., they have high variance, at the boundaries.\n",
    "\n",
    "Let's demonstrate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train = np.concatenate([\n",
    "    np.linspace(0, 1, 5),\n",
    "    np.linspace(1.5, 8.5, 40),\n",
    "    np.linspace(9, 10, 5)\n",
    "])\n",
    "y_train = np.sin(2*np.pi*x_train/10) + np.random.normal(0, 0.2, len(x_train))\n",
    "\n",
    "knots = np.linspace(2, 8, 8)\n",
    "\n",
    "model = RegressionSpline(degree=3)\n",
    "model.fit(x_train, y_train, knots)\n",
    "\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.6, s=40, label='Training data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_pred, 'b-', linewidth=2, label='Regression spline')\n",
    "ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "\n",
    "for knot in knots:\n",
    "    ax.axvline(knot, color='r', linestyle='--', alpha=0.2)\n",
    "\n",
    "ax.axvspan(0, 1.5, alpha=0.1, color='red', label='Boundary regions')\n",
    "ax.axvspan(8.5, 10, alpha=0.1, color='red')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Boundary Variance Problem in Regression Splines', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Polynomial Regression\n",
    "\n",
    "Splines are more flexible than global polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.25, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "knots = np.linspace(2, 8, 5)\n",
    "spline_model = RegressionSpline(degree=3)\n",
    "spline_model.fit(x_train, y_train, knots)\n",
    "y_spline = spline_model.predict(x_test)\n",
    "\n",
    "n_params_spline = len(knots) + 3 + 1\n",
    "poly_degree = n_params_spline - 1\n",
    "poly_coeffs = np.polyfit(x_train, y_train, poly_degree)\n",
    "y_poly = np.polyval(poly_coeffs, x_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=30, label='Data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_spline, 'b-', linewidth=2, label=f'Regression spline ({n_params_spline} params)')\n",
    "ax.plot(x_test, y_poly, 'r-', linewidth=2, label=f'Global polynomial (degree {poly_degree})', alpha=0.7)\n",
    "ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "\n",
    "for knot in knots:\n",
    "    ax.axvline(knot, color='b', linestyle='--', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Regression Spline vs Global Polynomial', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.show()\n",
    "\n",
    "mse_spline = mean_squared_error(y_train, spline_model.predict(x_train))\n",
    "mse_poly = mean_squared_error(y_train, np.polyval(poly_coeffs, x_train))\n",
    "\n",
    "print(f\"\\nTraining MSE:\")\n",
    "print(f\"  Spline: {mse_spline:.4f}\")\n",
    "print(f\"  Polynomial: {mse_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Natural Splines vs Regular Splines\n",
    "\n",
    "**Natural splines** address the boundary variance problem by:\n",
    "- Using degree $k$ polynomials between interior knots\n",
    "- Using degree $(k-1)/2$ polynomials beyond boundaries\n",
    "- For cubic natural splines (k=3): **linear beyond boundaries**\n",
    "\n",
    "A way to remedy this problem is to force the piecewise polynomial function to have a lower degree to the left of the leftmost knot, and to the right of the rightmost knot.\n",
    "\n",
    "**Key advantage**: Natural splines use only $m$ basis functions (vs $m+k+1$ for regular splines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Comparison: Regular vs Natural Cubic Splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train = np.concatenate([\n",
    "    np.array([0.0, 0.5, 1.0]),\n",
    "    np.linspace(2, 8, 35),\n",
    "    np.array([9.0, 9.5, 10.0])\n",
    "])\n",
    "y_train = np.sin(2*np.pi*x_train/10) + np.random.normal(0, 0.2, len(x_train))\n",
    "\n",
    "interior_knots = np.linspace(2, 8, 6)\n",
    "boundary_knots = np.array([0.0, 10.0])\n",
    "all_knots = np.sort(np.concatenate([boundary_knots, interior_knots]))\n",
    "\n",
    "regular_model = RegressionSpline(degree=3)\n",
    "regular_model.fit(x_train, y_train, interior_knots)\n",
    "\n",
    "natural_model = NaturalCubicSpline()\n",
    "natural_model.fit(x_train, y_train, all_knots)\n",
    "\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_regular = regular_model.predict(x_test)\n",
    "y_natural = natural_model.predict(x_test)\n",
    "y_true = np.sin(2*np.pi*x_test/10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=40, label='Training data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_regular, 'b-', linewidth=2.5, label='Regular cubic spline')\n",
    "ax.plot(x_test, y_true, 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "for knot in interior_knots:\n",
    "    ax.axvline(knot, color='r', linestyle='--', alpha=0.2)\n",
    "ax.axvspan(-0.5, 1.5, alpha=0.1, color='orange', label='Boundary regions')\n",
    "ax.axvspan(8.5, 10.5, alpha=0.1, color='orange')\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Regular Cubic Spline (may have high variance at boundaries)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=40, label='Training data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_natural, 'purple', linewidth=2.5, label='Natural cubic spline')\n",
    "ax.plot(x_test, y_true, 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "for knot in all_knots:\n",
    "    ax.axvline(knot, color='r', linestyle='--', alpha=0.2)\n",
    "ax.axvspan(-0.5, 1.5, alpha=0.1, color='lightblue', label='Linear beyond boundaries')\n",
    "ax.axvspan(8.5, 10.5, alpha=0.1, color='lightblue')\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Natural Cubic Spline (linear beyond boundaries → stable)', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Regular spline: {len(interior_knots) + 3 + 1} = {len(regular_model.coefficients)} parameters\")\n",
    "print(f\"Natural spline: {len(all_knots)} parameters (fewer!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom In on Boundary Behavior\n",
    "\n",
    "Let's examine the boundary regions more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "mask_left = x_test <= 2.5\n",
    "ax.scatter(x_train[x_train <= 2.5], y_train[x_train <= 2.5], \n",
    "          alpha=0.7, s=60, color='gray', zorder=3, label='Data')\n",
    "ax.plot(x_test[mask_left], y_regular[mask_left], 'b-', linewidth=3, label='Regular spline')\n",
    "ax.plot(x_test[mask_left], y_natural[mask_left], 'purple', linewidth=3, label='Natural spline', linestyle='--')\n",
    "ax.plot(x_test[mask_left], y_true[mask_left], 'g:', linewidth=2, label='True', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle=':', alpha=0.5, label='Boundary')\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Left Boundary (x ∈ [0, 2.5])', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "mask_right = x_test >= 7.5\n",
    "ax.scatter(x_train[x_train >= 7.5], y_train[x_train >= 7.5], \n",
    "          alpha=0.7, s=60, color='gray', zorder=3, label='Data')\n",
    "ax.plot(x_test[mask_right], y_regular[mask_right], 'b-', linewidth=3, label='Regular spline')\n",
    "ax.plot(x_test[mask_right], y_natural[mask_right], 'purple', linewidth=3, label='Natural spline', linestyle='--')\n",
    "ax.plot(x_test[mask_right], y_true[mask_right], 'g:', linewidth=2, label='True', alpha=0.7)\n",
    "ax.axvline(10, color='red', linestyle=':', alpha=0.5, label='Boundary')\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Right Boundary (x ∈ [7.5, 10])', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Reduction at Boundaries\n",
    "\n",
    "Demonstrate variance reduction through bootstrap resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 40\n",
    "x_base = np.concatenate([\n",
    "    np.array([0.0, 0.5, 1.0]),\n",
    "    np.linspace(2, 8, 30),\n",
    "    np.array([9.0, 9.5, 10.0])\n",
    "])\n",
    "true_func = lambda x: np.sin(2*np.pi*x/10)\n",
    "\n",
    "n_bootstrap = 50\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "y_regular_samples = []\n",
    "y_natural_samples = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    y_train = true_func(x_base) + np.random.normal(0, 0.2, len(x_base))\n",
    "    \n",
    "    regular_model = RegressionSpline(degree=3)\n",
    "    regular_model.fit(x_base, y_train, interior_knots)\n",
    "    y_regular_samples.append(regular_model.predict(x_test))\n",
    "    \n",
    "    natural_model = NaturalCubicSpline()\n",
    "    natural_model.fit(x_base, y_train, all_knots)\n",
    "    y_natural_samples.append(natural_model.predict(x_test))\n",
    "\n",
    "y_regular_samples = np.array(y_regular_samples)\n",
    "y_natural_samples = np.array(y_natural_samples)\n",
    "\n",
    "var_regular = np.var(y_regular_samples, axis=0)\n",
    "var_natural = np.var(y_natural_samples, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(x_test, var_regular, 'b-', linewidth=2, label='Regular spline variance')\n",
    "ax.plot(x_test, var_natural, 'purple', linewidth=2, label='Natural spline variance')\n",
    "\n",
    "ax.axvspan(0, 2, alpha=0.1, color='orange')\n",
    "ax.axvspan(8, 10, alpha=0.1, color='orange')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title(f'Prediction Variance from {n_bootstrap} Bootstrap Samples', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "boundary_mask = (x_test < 2) | (x_test > 8)\n",
    "interior_mask = (x_test >= 2) & (x_test <= 8)\n",
    "\n",
    "print(\"\\nAverage variance at BOUNDARIES:\")\n",
    "print(f\"  Regular: {np.mean(var_regular[boundary_mask]):.4f}\")\n",
    "print(f\"  Natural: {np.mean(var_natural[boundary_mask]):.4f}\")\n",
    "print(f\"  Reduction: {(1 - np.mean(var_natural[boundary_mask])/np.mean(var_regular[boundary_mask]))*100:.1f}%\")\n",
    "\n",
    "print(\"\\nAverage variance in INTERIOR:\")\n",
    "print(f\"  Regular: {np.mean(var_regular[interior_mask]):.4f}\")\n",
    "print(f\"  Natural: {np.mean(var_natural[interior_mask]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Efficiency\n",
    "\n",
    "Natural splines use fewer parameters while maintaining good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.2, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "n_knots_list = [4, 6, 8, 10]\n",
    "\n",
    "results = []\n",
    "for n_knots in n_knots_list:\n",
    "    interior_knots = np.linspace(2, 8, n_knots)\n",
    "    all_knots = np.sort(np.concatenate([[0, 10], interior_knots]))\n",
    "    \n",
    "    reg_model = RegressionSpline(degree=3)\n",
    "    reg_model.fit(x_train, y_train, interior_knots)\n",
    "    mse_reg = mean_squared_error(y_train, reg_model.predict(x_train))\n",
    "    n_params_reg = len(reg_model.coefficients)\n",
    "    \n",
    "    nat_model = NaturalCubicSpline()\n",
    "    nat_model.fit(x_train, y_train, all_knots)\n",
    "    mse_nat = mean_squared_error(y_train, nat_model.predict(x_train))\n",
    "    n_params_nat = len(nat_model.coefficients)\n",
    "    \n",
    "    results.append({\n",
    "        'n_knots': n_knots,\n",
    "        'regular_params': n_params_reg,\n",
    "        'natural_params': n_params_nat,\n",
    "        'regular_mse': mse_reg,\n",
    "        'natural_mse': mse_nat\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Knots':<10} {'Regular':<20} {'Natural':<20} {'MSE Comparison':<30}\")\n",
    "print(f\"{'(m)':<10} {'Params | MSE':<20} {'Params | MSE':<20} {'Regular vs Natural':<30}\")\n",
    "print(\"=\"*80)\n",
    "for r in results:\n",
    "    print(f\"{r['n_knots']:<10} \"\n",
    "          f\"{r['regular_params']:<7} | {r['regular_mse']:<11.4f} \"\n",
    "          f\"{r['natural_params']:<7} | {r['natural_mse']:<11.4f} \"\n",
    "          f\"{r['regular_mse']/r['natural_mse']:<30.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Smoothing Splines\n",
    "\n",
    "Smoothing splines solve the regularized problem:\n",
    "$$\\min_\\beta \\|y - G\\beta\\|_2^2 + \\lambda \\beta^T \\Omega \\beta$$\n",
    "\n",
    "where:\n",
    "- $G$: natural cubic spline basis with **knots at ALL training points** $x_1, \\dots, x_n$\n",
    "- $\\Omega_{ij} = \\int g''_i(t) g''_j(t) dt$: penalty matrix (penalizes curvature)\n",
    "- $\\lambda \\geq 0$: smoothing parameter\n",
    "\n",
    "**Solution**: $\\hat{\\beta} = (G^T G + \\lambda \\Omega)^{-1} G^T y$\n",
    "\n",
    "**Alternative formulation**: Minimize over ALL functions $f$:\n",
    "$$\\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\int (f''(x))^2 dx$$\n",
    "\n",
    "Smoothing splines circumvent the problem of knot selection (as they just use the inputs as knots), and simultaneously, they control for overfitting by shrinking the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Smoothing Parameter λ\n",
    "\n",
    "The parameter $\\lambda$ controls the bias-variance tradeoff:\n",
    "- $\\lambda \\to 0$: More flexible (low bias, high variance) - interpolates data\n",
    "- $\\lambda \\to \\infty$: More smooth (high bias, low variance) - approaches linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=30, noise_std=0.3, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_true = np.sin(2*np.pi*x_test/10)\n",
    "\n",
    "lambdas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "predictions = {}\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = SmoothingSpline(lambda_=lam)\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions[f'λ={lam}'] = model.predict(x_test)\n",
    "\n",
    "fig = plot_smoothing_comparison(\n",
    "    x_train, y_train, x_test, predictions,\n",
    "    y_true_func=lambda x: np.sin(2*np.pi*x/10),\n",
    "    title=\"Effect of Smoothing Parameter λ on Smoothing Splines\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"  Small λ → wiggly fit (overfitting)\")\n",
    "print(\"  Large λ → smooth fit (underfitting)\")\n",
    "print(\"  Need to select optimal λ via cross-validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed View: λ Effect on Individual Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_detail = [0.001, 0.1, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, lam in zip(axes, lambdas_detail):\n",
    "    model = SmoothingSpline(lambda_=lam)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    y_pred_train = model.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, y_pred_train)\n",
    "    r2 = r_squared(y_train, y_pred_train)\n",
    "    \n",
    "    ax.scatter(x_train, y_train, alpha=0.6, s=50, color='gray', zorder=3, label='Data')\n",
    "    ax.plot(x_test, y_pred, 'b-', linewidth=2.5, label='Smoothing spline')\n",
    "    ax.plot(x_test, y_true, 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=11)\n",
    "    ax.set_ylabel('y', fontsize=11)\n",
    "    ax.set_title(f'λ = {lam}\\nMSE={mse:.3f}, R²={r2:.3f}', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for λ Selection\n",
    "\n",
    "The optimal smoothing parameter is typically chosen via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=60, noise_std=0.25, x_range=(0, 10))\n",
    "\n",
    "lambdas_cv = np.logspace(-3, 2, 30)\n",
    "\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "model = SmoothingSpline()\n",
    "best_lambda, cv_errors = model.cross_validate(x_train, y_train, lambdas_cv, cv_folds=5)\n",
    "\n",
    "print(f\"\\nOptimal λ = {best_lambda:.4f}\")\n",
    "print(f\"CV error at optimal λ = {cv_errors[np.argmin(cv_errors)]:.4f}\")\n",
    "\n",
    "fig = plot_cv_curve(lambdas_cv, cv_errors, best_lambda,\n",
    "                   title=\"5-Fold Cross-Validation for Smoothing Parameter Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit with Optimal λ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = SmoothingSpline(lambda_=best_lambda)\n",
    "final_model.fit(x_train, y_train)\n",
    "\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_pred = final_model.predict(x_test)\n",
    "y_true = np.sin(2*np.pi*x_test/10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=40, label='Training data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_pred, 'b-', linewidth=2.5, label=f'Smoothing spline (λ={best_lambda:.4f})')\n",
    "ax.plot(x_test, y_true, 'g--', alpha=0.6, linewidth=2, label='True function')\n",
    "\n",
    "for xi in x_train[::5]:\n",
    "    ax.axvline(xi, color='red', linestyle=':', alpha=0.1)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title(f'Smoothing Spline with Optimal λ (selected by CV)\\n'\n",
    "            f'Knots placed at ALL {len(x_train)} training points', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "y_pred_train = final_model.predict(x_train)\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"  Training MSE: {mean_squared_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  Training R²: {r_squared(y_train, y_pred_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Minimization Perspective\n",
    "\n",
    "Smoothing splines can be derived from minimizing:\n",
    "$$\\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\int (f''(x))^2 dx$$\n",
    "\n",
    "Let's visualize the components of this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=20, noise_std=0.2, x_range=(0, 10))\n",
    "\n",
    "lambdas_viz = [0.001, 0.1, 10.0]\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "results = []\n",
    "for lam in lambdas_viz:\n",
    "    model = SmoothingSpline(lambda_=lam)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    data_fit = np.sum((y_train - y_pred_train)**2)\n",
    "    \n",
    "    second_deriv = np.diff(y_pred, n=2) / (x_test[1] - x_test[0])**2\n",
    "    roughness = np.sum(second_deriv**2) * (x_test[1] - x_test[0])\n",
    "    \n",
    "    results.append({\n",
    "        'lambda': lam,\n",
    "        'data_fit': data_fit,\n",
    "        'roughness': roughness,\n",
    "        'objective': data_fit + lam * roughness\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'λ':<12} {'Data Fit':<15} {'Roughness':<15} {'Total Objective':<15}\")\n",
    "print(f\"{'(param)':<12} {'∑(y-f(x))²':<15} {'∫(f\\'\\')² dx':<15} {'(approx)':<15}\")\n",
    "print(\"=\"*70)\n",
    "for r in results:\n",
    "    print(f\"{r['lambda']:<12.3f} {r['data_fit']:<15.2f} {r['roughness']:<15.2f} {r['objective']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Automatic vs Manual Knot Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=40, noise_std=0.25, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "manual_knots = np.linspace(2, 8, 6)\n",
    "reg_model = RegressionSpline(degree=3)\n",
    "reg_model.fit(x_train, y_train, manual_knots)\n",
    "y_reg = reg_model.predict(x_test)\n",
    "\n",
    "smooth_model = SmoothingSpline(lambda_=0.1)\n",
    "smooth_model.fit(x_train, y_train)\n",
    "y_smooth = smooth_model.predict(x_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=40, label='Data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_reg, 'b-', linewidth=2, label=f'Regression spline ({len(manual_knots)} manual knots)', alpha=0.7)\n",
    "ax.plot(x_test, y_smooth, 'purple', linewidth=2, label=f'Smoothing spline ({len(x_train)} automatic knots)', linestyle='--')\n",
    "ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g:', linewidth=2, label='True function', alpha=0.5)\n",
    "\n",
    "for knot in manual_knots:\n",
    "    ax.axvline(knot, color='blue', linestyle='--', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Manual Knot Placement vs Automatic (Smoothing Spline)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Basis Functions:\n",
    "1. **Truncated power basis** provides a natural parametrization for splines\n",
    "2. For degree $k$ with $m$ knots, we need $m + k + 1$ basis functions\n",
    "3. The basis consists of:\n",
    "   - Global polynomial terms: $1, x, x^2, \\dots, x^k$\n",
    "   - Local truncated terms: $(x - t_j)_+^k$ that \"activate\" at each knot\n",
    "4. Higher degree → more basis functions → more flexible splines\n",
    "5. Cubic splines (degree 3) are most common in practice\n",
    "\n",
    "### Regression Splines:\n",
    "1. **Regression splines** fit splines to data via least squares: $\\hat{\\beta} = (G^T G)^{-1} G^T y$\n",
    "2. More knots → more flexibility → better training fit (but potential overfitting)\n",
    "3. Higher polynomial degree → smoother derivatives but more parameters\n",
    "4. **Main limitation**: High variance at boundaries (solved by natural splines)\n",
    "5. **Splines vs polynomials**: Splines offer local control, polynomials are global\n",
    "\n",
    "### Natural Splines:\n",
    "1. **Natural splines** are linear beyond boundary knots → reduced variance at boundaries\n",
    "2. Use only $m$ basis functions (vs $m+k+1$ for regular splines) → more parameter efficient\n",
    "3. Provide more **stable extrapolation** beyond the data range\n",
    "4. Particularly useful when data is sparse near boundaries\n",
    "5. For cubic natural splines: linear extrapolation is often more reasonable than cubic\n",
    "\n",
    "### Smoothing Splines:\n",
    "1. **Smoothing splines** eliminate knot selection by using ALL training points as knots\n",
    "2. **Regularization** via $\\lambda \\beta^T \\Omega \\beta$ prevents overfitting\n",
    "3. Smoothing parameter $\\lambda$ controls bias-variance tradeoff:\n",
    "   - Small $\\lambda$ → flexible, wiggly fit\n",
    "   - Large $\\lambda$ → smooth, rigid fit\n",
    "4. Optimal $\\lambda$ chosen via **cross-validation**\n",
    "5. Equivalent to functional minimization: $\\min_f \\sum(y_i - f(x_i))^2 + \\lambda \\int(f'')^2 dx$\n",
    "6. **Advantages over regression splines**:\n",
    "   - Automatic knot placement\n",
    "   - Only one tuning parameter (λ)\n",
    "   - Computationally efficient with B-spline basis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
