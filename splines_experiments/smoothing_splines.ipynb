{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing Splines\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Smoothing splines solve the regularized problem:\n",
    "$$\\min_\\beta \\|y - G\\beta\\|_2^2 + \\lambda \\beta^T \\Omega \\beta$$\n",
    "\n",
    "where:\n",
    "- $G$: natural cubic spline basis with **knots at ALL training points** $x_1, \\dots, x_n$\n",
    "- $\\Omega_{ij} = \\int g''_i(t) g''_j(t) dt$: penalty matrix (penalizes curvature)\n",
    "- $\\lambda \\geq 0$: smoothing parameter\n",
    "\n",
    "**Solution**: $\\hat{\\beta} = (G^T G + \\lambda \\Omega)^{-1} G^T y$\n",
    "\n",
    "**Alternative formulation**: Minimize over ALL functions $f$:\n",
    "$$\\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\int (f''(x))^2 dx$$\n",
    "\n",
    "Smoothing splines circumvent the problem of knot selection (as they just use the inputs as knots), and simultaneously, they control for overfitting by shrinking the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from splines import SmoothingSpline\n",
    "from utils import (\n",
    "    generate_sinusoidal_data,\n",
    "    plot_smoothing_comparison,\n",
    "    plot_cv_curve,\n",
    "    mean_squared_error,\n",
    "    r_squared\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Effect of Smoothing Parameter λ\n",
    "\n",
    "The parameter $\\lambda$ controls the bias-variance tradeoff:\n",
    "- $\\lambda \\to 0$: More flexible (low bias, high variance) - interpolates data\n",
    "- $\\lambda \\to \\infty$: More smooth (high bias, low variance) - approaches linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=30, noise_std=0.3, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_true = np.sin(2*np.pi*x_test/10)\n",
    "\n",
    "# Try different lambda values\n",
    "lambdas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "predictions = {}\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = SmoothingSpline(lambda_=lam)\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions[f'λ={lam}'] = model.predict(x_test)\n",
    "\n",
    "# Plot comparison\n",
    "fig = plot_smoothing_comparison(\n",
    "    x_train, y_train, x_test, predictions,\n",
    "    y_true_func=lambda x: np.sin(2*np.pi*x/10),\n",
    "    title=\"Effect of Smoothing Parameter λ on Smoothing Splines\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"  Small λ → wiggly fit (overfitting)\")\n",
    "print(\"  Large λ → smooth fit (underfitting)\")\n",
    "print(\"  Need to select optimal λ via cross-validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detailed View: λ Effect on Individual Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data\n",
    "lambdas_detail = [0.001, 0.1, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, lam in zip(axes, lambdas_detail):\n",
    "    model = SmoothingSpline(lambda_=lam)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Compute metrics\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, y_pred_train)\n",
    "    r2 = r_squared(y_train, y_pred_train)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x_train, y_train, alpha=0.6, s=50, color='gray', zorder=3, label='Data')\n",
    "    ax.plot(x_test, y_pred, 'b-', linewidth=2.5, label='Smoothing spline')\n",
    "    ax.plot(x_test, y_true, 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=11)\n",
    "    ax.set_ylabel('y', fontsize=11)\n",
    "    ax.set_title(f'λ = {lam}\\nMSE={mse:.3f}, R²={r2:.3f}', fontsize=12)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation for λ Selection\n",
    "\n",
    "The optimal smoothing parameter is typically chosen via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=60, noise_std=0.25, x_range=(0, 10))\n",
    "\n",
    "# Define candidate lambda values (log-scale)\n",
    "lambdas_cv = np.logspace(-3, 2, 30)\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "model = SmoothingSpline()\n",
    "best_lambda, cv_errors = model.cross_validate(x_train, y_train, lambdas_cv, cv_folds=5)\n",
    "\n",
    "print(f\"\\nOptimal λ = {best_lambda:.4f}\")\n",
    "print(f\"CV error at optimal λ = {cv_errors[np.argmin(cv_errors)]:.4f}\")\n",
    "\n",
    "# Plot CV curve\n",
    "fig = plot_cv_curve(lambdas_cv, cv_errors, best_lambda,\n",
    "                   title=\"5-Fold Cross-Validation for Smoothing Parameter Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit with Optimal λ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit final model with optimal lambda\n",
    "final_model = SmoothingSpline(lambda_=best_lambda)\n",
    "final_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_pred = final_model.predict(x_test)\n",
    "y_true = np.sin(2*np.pi*x_test/10)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=40, label='Training data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_pred, 'b-', linewidth=2.5, label=f'Smoothing spline (λ={best_lambda:.4f})')\n",
    "ax.plot(x_test, y_true, 'g--', alpha=0.6, linewidth=2, label='True function')\n",
    "\n",
    "# Mark training points (automatic knots)\n",
    "for xi in x_train[::5]:  # Show every 5th for clarity\n",
    "    ax.axvline(xi, color='red', linestyle=':', alpha=0.1)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title(f'Smoothing Spline with Optimal λ (selected by CV)\\n'\n",
    "            f'Knots placed at ALL {len(x_train)} training points', fontsize=13)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "y_pred_train = final_model.predict(x_train)\n",
    "print(f\"\\nFinal model performance:\")\n",
    "print(f\"  Training MSE: {mean_squared_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  Training R²: {r_squared(y_train, y_pred_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Functional Minimization Perspective\n",
    "\n",
    "Smoothing splines can be derived from minimizing:\n",
    "$$\\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\int (f''(x))^2 dx$$\n",
    "\n",
    "Let's visualize the components of this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple data\n",
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=20, noise_std=0.2, x_range=(0, 10))\n",
    "\n",
    "# Fit models with different lambdas\n",
    "lambdas_viz = [0.001, 0.1, 10.0]\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "results = []\n",
    "for lam in lambdas_viz:\n",
    "    model = SmoothingSpline(lambda_=lam)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    # Data fit term\n",
    "    data_fit = np.sum((y_train - y_pred_train)**2)\n",
    "    \n",
    "    # Roughness penalty (approximate via finite differences)\n",
    "    second_deriv = np.diff(y_pred, n=2) / (x_test[1] - x_test[0])**2\n",
    "    roughness = np.sum(second_deriv**2) * (x_test[1] - x_test[0])\n",
    "    \n",
    "    results.append({\n",
    "        'lambda': lam,\n",
    "        'data_fit': data_fit,\n",
    "        'roughness': roughness,\n",
    "        'objective': data_fit + lam * roughness\n",
    "    })\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'λ':<12} {'Data Fit':<15} {'Roughness':<15} {'Total Objective':<15}\")\n",
    "print(f\"{'(param)':<12} {'∑(y-f(x))²':<15} {'∫(f\\'\\')² dx':<15} {'(approx)':<15}\")\n",
    "print(\"=\"*70)\n",
    "for r in results:\n",
    "    print(f\"{r['lambda']:<12.3f} {r['data_fit']:<15.2f} {r['roughness']:<15.2f} {r['objective']:<15.2f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTrade-off: Data fit vs Roughness\")\n",
    "print(\"  Small λ → prioritize data fit (wiggly curve, high roughness)\")\n",
    "print(\"  Large λ → prioritize smoothness (poor data fit, low roughness)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Automatic vs Manual Knot Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splines import RegressionSpline\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(123)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=40, noise_std=0.25, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "# Method 1: Regression spline with manual knots\n",
    "manual_knots = np.linspace(2, 8, 6)\n",
    "reg_model = RegressionSpline(degree=3)\n",
    "reg_model.fit(x_train, y_train, manual_knots)\n",
    "y_reg = reg_model.predict(x_test)\n",
    "\n",
    "# Method 2: Smoothing spline with automatic knots\n",
    "smooth_model = SmoothingSpline(lambda_=0.1)\n",
    "smooth_model.fit(x_train, y_train)\n",
    "y_smooth = smooth_model.predict(x_test)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=40, label='Data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_reg, 'b-', linewidth=2, label=f'Regression spline ({len(manual_knots)} manual knots)', alpha=0.7)\n",
    "ax.plot(x_test, y_smooth, 'purple', linewidth=2, label=f'Smoothing spline ({len(x_train)} automatic knots)', linestyle='--')\n",
    "ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g:', linewidth=2, label='True function', alpha=0.5)\n",
    "\n",
    "# Show manual knots\n",
    "for knot in manual_knots:\n",
    "    ax.axvline(knot, color='blue', linestyle='--', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Manual Knot Placement vs Automatic (Smoothing Spline)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Smoothing splines: No need to choose knot locations!\")\n",
    "print(\"  → Uses ALL training points as knots\")\n",
    "print(\"  → Controls overfitting via regularization (λ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Smoothing splines** eliminate knot selection by using ALL training points as knots\n",
    "2. **Regularization** via $\\lambda \\beta^T \\Omega \\beta$ prevents overfitting\n",
    "3. Smoothing parameter $\\lambda$ controls bias-variance tradeoff:\n",
    "   - Small $\\lambda$ → flexible, wiggly fit\n",
    "   - Large $\\lambda$ → smooth, rigid fit\n",
    "4. Optimal $\\lambda$ chosen via **cross-validation**\n",
    "5. Equivalent to functional minimization: $\\min_f \\sum(y_i - f(x_i))^2 + \\lambda \\int(f'')^2 dx$\n",
    "6. **Advantages over regression splines**:\n",
    "   - Automatic knot placement\n",
    "   - Only one tuning parameter (λ)\n",
    "   - Computationally efficient with B-spline basis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
