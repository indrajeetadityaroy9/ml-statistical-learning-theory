{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import BSpline\n",
    "from scipy.linalg import svd\n",
    "from splines import bspline_basis, PenalizedSpline, RegressionSpline, NaturalCubicSpline\n",
    "from gam import GAM\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spline Basis Functions\n",
    "\n",
    "### 1.1 Truncated Power Basis\n",
    "\n",
    "A **truncated power basis** of degree $d$ with knots $\\xi_1, \\ldots, \\xi_K$ consists of:\n",
    "\n",
    "$$1, x, x^2, \\ldots, x^d, (x - \\xi_1)_+^d, (x - \\xi_2)_+^d, \\ldots, (x - \\xi_K)_+^d$$\n",
    "\n",
    "where $(u)_+ = \\max(0, u)$.\n",
    "\n",
    "**Properties**:\n",
    "- Simple to understand\n",
    "- Numerically unstable for high degrees\n",
    "- Poor conditioning of design matrix\n",
    "\n",
    "### 1.2 B-Spline Basis\n",
    "\n",
    "**B-splines** provide a numerically stable alternative with **local support**.\n",
    "\n",
    "For degree $d$ and knot sequence $t_1, \\ldots, t_m$, B-splines are defined recursively:\n",
    "\n",
    "$$B_{i,0}(x) = \\begin{cases} 1 & \\text{if } t_i \\leq x < t_{i+1} \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "$$B_{i,d}(x) = \\frac{x - t_i}{t_{i+d} - t_i} B_{i,d-1}(x) + \\frac{t_{i+d+1} - x}{t_{i+d+1} - t_{i+1}} B_{i+1,d-1}(x)$$\n",
    "\n",
    "**Key Properties**:\n",
    "1. **Partition of unity**: $\\sum_{i=1}^{m} B_{i,d}(x) = 1$ for all $x$\n",
    "2. **Non-negativity**: $B_{i,d}(x) \\geq 0$\n",
    "3. **Local support**: $B_{i,d}(x) = 0$ outside $[t_i, t_{i+d+1}]$\n",
    "4. **Numerical stability**: Well-conditioned basis matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0, 10, 500)\n",
    "knots = np.array([2, 4, 6, 8])\n",
    "degree = 3\n",
    "\n",
    "B = bspline_basis(x_grid, knots, degree=degree)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for j in range(B.shape[1]):\n",
    "    axes[0].plot(x_grid, B[:, j], linewidth=2, label=f'$B_{{{j+1},{degree}}}(x)$')\n",
    "\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "for knot in knots:\n",
    "    axes[0].axvline(knot, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('x', fontsize=12)\n",
    "axes[0].set_ylabel('Basis Function Value', fontsize=12)\n",
    "axes[0].set_title(f'B-Spline Basis Functions (degree={degree})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(ncol=2, fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "row_sums = np.sum(B, axis=1)\n",
    "axes[1].plot(x_grid, row_sums, linewidth=3, color='darkred')\n",
    "axes[1].axhline(1, color='blue', linestyle='--', linewidth=2, label='y = 1')\n",
    "axes[1].set_xlabel('x', fontsize=12)\n",
    "axes[1].set_ylabel(r'$\\sum_{i} B_{i,d}(x)$', fontsize=12)\n",
    "axes[1].set_title('Partition of Unity Property', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0.95, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ B-spline basis matrix shape: {B.shape}\")\n",
    "print(f\"✓ Number of basis functions: {B.shape[1]} = {len(knots)} knots + {degree} + 1\")\n",
    "print(f\"✓ Partition of unity: min={row_sums.min():.10f}, max={row_sums.max():.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Penalized Regression Framework\n",
    "\n",
    "### 2.1 Smoothing Splines\n",
    "\n",
    "**Smoothing splines** minimize the penalized residual sum of squares:\n",
    "\n",
    "$$\\hat{f} = \\arg\\min_{f \\in \\mathcal{H}} \\sum_{i=1}^{n} (y_i - f(x_i))^2 + \\lambda \\int [f''(x)]^2 dx$$\n",
    "\n",
    "where:\n",
    "- First term: fidelity to data\n",
    "- Second term: penalty on curvature\n",
    "- $\\lambda \\geq 0$: smoothing parameter\n",
    "\n",
    "**Solution**: Natural cubic spline with knots at each unique $x_i$.\n",
    "\n",
    "### 2.2 Penalized Splines (P-Splines)\n",
    "\n",
    "**P-splines** (Eilers & Marx, 1996) use B-spline basis with **difference penalties**:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg\\min_{\\beta} \\|y - B\\beta\\|^2 + \\lambda \\beta^T P \\beta$$\n",
    "\n",
    "where $P = D^T D$ is the penalty matrix and $D$ is the $m$-th order difference operator.\n",
    "\n",
    "**Difference Operators**:\n",
    "- First order ($m=1$): Penalizes jumps in $\\beta$ (roughness of $f$)\n",
    "- Second order ($m=2$): Penalizes curvature (most common)\n",
    "- Third order ($m=3$): Penalizes rate of curvature change\n",
    "\n",
    "**Closed-form solution**:\n",
    "\n",
    "$$\\hat{\\beta} = (B^T B + \\lambda P)^{-1} B^T y$$\n",
    "\n",
    "**Hat matrix**:\n",
    "\n",
    "$$H = B(B^T B + \\lambda P)^{-1} B^T$$\n",
    "\n",
    "with $\\hat{y} = Hy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_basis = 8\n",
    "\n",
    "def difference_matrix(n, order):\n",
    "    D = np.eye(n)\n",
    "    for _ in range(order):\n",
    "        D = np.diff(D, axis=0)\n",
    "    return D\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for i, order in enumerate([1, 2, 3]):\n",
    "    D = difference_matrix(n_basis, order)\n",
    "    P = D.T @ D\n",
    "    \n",
    "    sns.heatmap(P, annot=True, fmt='.0f', cmap='RdBu_r', center=0, \n",
    "                cbar_kws={'label': 'Value'}, ax=axes[i])\n",
    "    axes[i].set_title(f'Penalty Matrix P (order={order})\\nShape: {P.shape}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Coefficient Index')\n",
    "    axes[i].set_ylabel('Coefficient Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Penalty Matrix Properties:\")\n",
    "print(\"=\" * 60)\n",
    "for order in [1, 2, 3]:\n",
    "    D = difference_matrix(n_basis, order)\n",
    "    P = D.T @ D\n",
    "    rank = np.linalg.matrix_rank(P)\n",
    "    print(f\"Order {order}: Shape {P.shape}, Rank {rank}, Null space dim = {n_basis - rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effective Degrees of Freedom\n",
    "\n",
    "### 3.1 Definition\n",
    "\n",
    "The **effective degrees of freedom (edf)** quantifies model complexity:\n",
    "\n",
    "$$\\text{df}(\\lambda) = \\text{tr}(H) = \\text{tr}\\left(B(B^T B + \\lambda P)^{-1} B^T\\right)$$\n",
    "\n",
    "**Properties**:\n",
    "- When $\\lambda = 0$: $\\text{df} = p$ (number of basis functions)\n",
    "- When $\\lambda \\to \\infty$: $\\text{df} \\to \\text{rank}(P^\\perp)$ (null space dimension)\n",
    "- Monotone decreasing in $\\lambda$\n",
    "\n",
    "### 3.2 Bayesian Interpretation\n",
    "\n",
    "P-splines have a **Bayesian interpretation**:\n",
    "\n",
    "$$p(\\beta \\mid \\sigma^2, \\lambda) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\beta^T P \\beta\\right)$$\n",
    "\n",
    "This is a Gaussian prior with precision matrix $P/\\sigma^2$.\n",
    "\n",
    "The posterior mean is:\n",
    "\n",
    "$$E[\\beta \\mid y] = (B^T B + \\lambda P)^{-1} B^T y$$\n",
    "\n",
    "which is **identical** to the penalized least squares solution!\n",
    "\n",
    "### 3.3 Variance Estimation\n",
    "\n",
    "**Residual variance**:\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{\\|y - \\hat{y}\\|^2}{n - \\text{df}(\\lambda)}$$\n",
    "\n",
    "**Covariance of coefficients**:\n",
    "\n",
    "$$\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (B^T B + \\lambda P)^{-1} B^T B (B^T B + \\lambda P)^{-1}$$\n",
    "\n",
    "**Pointwise standard errors**:\n",
    "\n",
    "$$\\text{se}(\\hat{f}(x_0)) = \\sqrt{b(x_0)^T \\text{Cov}(\\hat{\\beta}) \\, b(x_0)}$$\n",
    "\n",
    "where $b(x_0)$ is the B-spline basis vector at $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.linspace(0, 10, n)\n",
    "y = np.sin(x) + np.random.normal(0, 0.3, n)\n",
    "\n",
    "lambdas = np.logspace(-3, 3, 50)\n",
    "edfs = []\n",
    "gcv_scores = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = PenalizedSpline(n_knots=20, degree=3, lambda_=lam, diff_order=2)\n",
    "    model.fit(x, y)\n",
    "    edfs.append(model.effective_df())\n",
    "    gcv_scores.append(model.gcv_score())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].semilogx(lambdas, edfs, linewidth=3, color='darkblue')\n",
    "axes[0].axhline(len(knots) + degree + 1, color='red', linestyle='--', \n",
    "                label=f'Max DF (λ=0): {len(knots) + degree + 1}', linewidth=2)\n",
    "axes[0].set_xlabel('Smoothing Parameter λ (log scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Effective Degrees of Freedom', fontsize=12)\n",
    "axes[0].set_title('Effective DF vs Smoothing Parameter', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogx(lambdas, gcv_scores, linewidth=3, color='darkgreen')\n",
    "min_idx = np.argmin(gcv_scores)\n",
    "axes[1].scatter(lambdas[min_idx], gcv_scores[min_idx], s=200, color='red', \n",
    "                zorder=5, label=f'Optimal λ={lambdas[min_idx]:.3f}')\n",
    "axes[1].set_xlabel('Smoothing Parameter λ (log scale)', fontsize=12)\n",
    "axes[1].set_ylabel('GCV Score', fontsize=12)\n",
    "axes[1].set_title('GCV Model Selection', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Effective Degrees of Freedom Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"λ = {lambdas[0]:.4f} (small):  edf = {edfs[0]:.2f}\")\n",
    "print(f\"λ = {lambdas[-1]:.4f} (large): edf = {edfs[-1]:.2f}\")\n",
    "print(f\"\\nOptimal λ = {lambdas[min_idx]:.4f}: edf = {edfs[min_idx]:.2f}\")\n",
    "print(f\"\\n✓ As λ increases, effective DF decreases monotonically\")\n",
    "print(f\"✓ GCV balances fit and complexity to select optimal λ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection: Generalized Cross-Validation\n",
    "\n",
    "### 4.1 GCV Criterion\n",
    "\n",
    "**Generalized Cross-Validation (GCV)** provides automatic smoothing parameter selection:\n",
    "\n",
    "$$\\text{GCV}(\\lambda) = \\frac{n \\cdot \\text{RSS}(\\lambda)}{[n - \\text{df}(\\lambda)]^2}$$\n",
    "\n",
    "where $\\text{RSS}(\\lambda) = \\|y - \\hat{y}_\\lambda\\|^2$.\n",
    "\n",
    "**Interpretation**: Approximation to leave-one-out cross-validation without actually refitting $n$ times.\n",
    "\n",
    "**Alternative: AIC and BIC**\n",
    "\n",
    "$$\\text{AIC}(\\lambda) = n \\log(\\text{RSS}(\\lambda)/n) + 2 \\cdot \\text{df}(\\lambda)$$\n",
    "\n",
    "$$\\text{BIC}(\\lambda) = n \\log(\\text{RSS}(\\lambda)/n) + \\log(n) \\cdot \\text{df}(\\lambda)$$\n",
    "\n",
    "### 4.2 K-Fold Cross-Validation\n",
    "\n",
    "For finite samples, **K-fold CV** provides unbiased assessment:\n",
    "\n",
    "$$\\text{CV}_K(\\lambda) = \\frac{1}{n} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{F}_k} (y_i - \\hat{f}_{-k}(x_i; \\lambda))^2$$\n",
    "\n",
    "where $\\hat{f}_{-k}$ is fitted without fold $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 150\n",
    "x_train = np.sort(np.random.uniform(0, 10, n))\n",
    "y_train = 2 * np.sin(x_train) + 0.5 * x_train + np.random.normal(0, 0.5, n)\n",
    "\n",
    "lambdas = np.logspace(-2, 2, 30)\n",
    "gcv_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    model = PenalizedSpline(n_knots=15, degree=3, lambda_=lam, diff_order=2)\n",
    "    model.fit(x_train, y_train)\n",
    "    gcv_scores.append(model.gcv_score())\n",
    "    \n",
    "    _, cv_err = model.cross_validate(x_train, y_train, [lam], cv_folds=5)\n",
    "    cv_scores.append(cv_err[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.semilogx(lambdas, gcv_scores, 'o-', linewidth=2.5, markersize=7, \n",
    "            label='GCV', color='steelblue')\n",
    "ax.semilogx(lambdas, cv_scores, 's-', linewidth=2.5, markersize=7, \n",
    "            label='5-Fold CV', color='darkred')\n",
    "\n",
    "gcv_min_idx = np.argmin(gcv_scores)\n",
    "cv_min_idx = np.argmin(cv_scores)\n",
    "ax.scatter(lambdas[gcv_min_idx], gcv_scores[gcv_min_idx], s=250, \n",
    "           color='steelblue', marker='*', zorder=5, edgecolor='black', linewidth=2)\n",
    "ax.scatter(lambdas[cv_min_idx], cv_scores[cv_min_idx], s=250, \n",
    "           color='darkred', marker='*', zorder=5, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Smoothing Parameter λ (log scale)', fontsize=12)\n",
    "ax.set_ylabel('Prediction Error', fontsize=12)\n",
    "ax.set_title('GCV vs K-Fold Cross-Validation', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model Selection Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"GCV optimal λ:     {lambdas[gcv_min_idx]:.4f}\")\n",
    "print(f\"5-Fold CV optimal λ: {lambdas[cv_min_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generalized Additive Models: Backfitting Algorithm\n",
    "\n",
    "### 5.1 GAM Framework\n",
    "\n",
    "**Generalized Additive Model**:\n",
    "\n",
    "$$E(Y \\mid X) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p)$$\n",
    "\n",
    "with constraint $E[f_j(X_j)] = 0$ for identifiability.\n",
    "\n",
    "**Penalized least squares**:\n",
    "\n",
    "$$\\min_{\\beta_0, f_1, \\ldots, f_p} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p} f_j(x_{ij})\\right)^2 + \\sum_{j=1}^{p} \\lambda_j \\int [f_j''(x)]^2 dx$$\n",
    "\n",
    "### 5.2 Backfitting Algorithm\n",
    "\n",
    "**Initialize**: $\\hat{\\beta}_0 = \\bar{y}$, $\\hat{f}_j = 0$ for all $j$.\n",
    "\n",
    "**Cycle** until convergence:\n",
    "\n",
    "For $j = 1, 2, \\ldots, p$:\n",
    "1. Compute **partial residuals**:\n",
    "   $$r_j = y - \\hat{\\beta}_0 - \\sum_{k \\neq j} \\hat{f}_k(x_k)$$\n",
    "\n",
    "2. Fit $\\hat{f}_j$ using P-spline smoother:\n",
    "   $$\\hat{f}_j \\leftarrow S_j(r_j)$$\n",
    "   where $S_j$ is the P-spline smoother for feature $j$.\n",
    "\n",
    "3. **Center** the function:\n",
    "   $$\\hat{f}_j \\leftarrow \\hat{f}_j - \\frac{1}{n}\\sum_{i=1}^{n} \\hat{f}_j(x_{ij})$$\n",
    "\n",
    "**Convergence**: Check $\\max_j \\|\\hat{f}_j^{\\text{new}} - \\hat{f}_j^{\\text{old}}\\| < \\epsilon$.\n",
    "\n",
    "### 5.3 Effective Degrees of Freedom for GAMs\n",
    "\n",
    "Total effective DF:\n",
    "\n",
    "$$\\text{df}_{\\text{total}} = 1 + \\sum_{j=1}^{p} \\text{df}_j$$\n",
    "\n",
    "where $\\text{df}_j = \\text{tr}(S_j)$ for each smooth term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "x1 = np.random.uniform(-2, 2, n)\n",
    "x2 = np.random.uniform(-2, 2, n)\n",
    "f1_true = lambda x: 2 * np.sin(np.pi * x)\n",
    "f2_true = lambda x: x**2 - 1\n",
    "y = f1_true(x1) + f2_true(x2) + np.random.normal(0, 0.5, n)\n",
    "\n",
    "X = np.column_stack([x1, x2])\n",
    "\n",
    "class GAMWithTracking(GAM):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.iteration_errors = []\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        super().fit(X, y, verbose=verbose)\n",
    "\n",
    "gam = GAM(smooth_features=[0, 1], n_knots=15, lambda_=1.0, max_iter=50, tol=1e-6)\n",
    "gam.fit(X, y, verbose=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "x1_grid = np.linspace(-2, 2, 100)\n",
    "x1_vals, f1_est = gam.get_smooth_function(0, n_points=100)\n",
    "axes[0].plot(x1_grid, f1_true(x1_grid), 'k--', linewidth=3, label='True f₁(x₁)', alpha=0.7)\n",
    "axes[0].plot(x1_vals, f1_est, linewidth=3, color='darkblue', label='Estimated f̂₁(x₁)')\n",
    "axes[0].scatter(x1, y - f2_true(x2), alpha=0.3, s=20, color='gray', label='Partial residuals')\n",
    "axes[0].set_xlabel('x₁', fontsize=12)\n",
    "axes[0].set_ylabel('f₁(x₁)', fontsize=12)\n",
    "axes[0].set_title('Component Function 1', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "x2_grid = np.linspace(-2, 2, 100)\n",
    "x2_vals, f2_est = gam.get_smooth_function(1, n_points=100)\n",
    "axes[1].plot(x2_grid, f2_true(x2_grid), 'k--', linewidth=3, label='True f₂(x₂)', alpha=0.7)\n",
    "axes[1].plot(x2_vals, f2_est, linewidth=3, color='darkred', label='Estimated f̂₂(x₂)')\n",
    "axes[1].scatter(x2, y - f1_true(x1), alpha=0.3, s=20, color='gray', label='Partial residuals')\n",
    "axes[1].set_xlabel('x₂', fontsize=12)\n",
    "axes[1].set_ylabel('f₂(x₂)', fontsize=12)\n",
    "axes[1].set_title('Component Function 2', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary = gam.summary()\n",
    "print(\"\\nGAM Backfitting Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Intercept: {summary['intercept']:.3f}\")\n",
    "print(f\"Total Effective DF: {summary['total_edf']:.1f}\")\n",
    "for feat_idx, info in summary['smooth_terms'].items():\n",
    "    print(f\"  Feature {feat_idx+1}: edf = {info['edf']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Intervals and Inference\n",
    "\n",
    "### 6.1 Pointwise Confidence Bands\n",
    "\n",
    "For a fitted smooth $\\hat{f}(x)$, the **pointwise standard error** is:\n",
    "\n",
    "$$\\text{se}(\\hat{f}(x_0)) = \\hat{\\sigma} \\sqrt{b(x_0)^T (B^T B + \\lambda P)^{-1} B^T B (B^T B + \\lambda P)^{-1} b(x_0)}$$\n",
    "\n",
    "**95% Pointwise Confidence Interval**:\n",
    "\n",
    "$$\\hat{f}(x_0) \\pm 1.96 \\cdot \\text{se}(\\hat{f}(x_0))$$\n",
    "\n",
    "### 6.2 Simultaneous Confidence Bands\n",
    "\n",
    "For **simultaneous coverage**, use critical value from simulation:\n",
    "\n",
    "$$\\hat{f}(x_0) \\pm c_{\\alpha} \\cdot \\text{se}(\\hat{f}(x_0))$$\n",
    "\n",
    "where $c_{\\alpha}$ ensures $P(\\hat{f}(x) - c_{\\alpha} \\cdot \\text{se}(\\hat{f}(x)) \\leq f(x) \\leq \\hat{f}(x) + c_{\\alpha} \\cdot \\text{se}(\\hat{f}(x)) \\ \\forall x) = 1 - \\alpha$.\n",
    "\n",
    "### 6.3 Testing for Non-linearity\n",
    "\n",
    "**Null hypothesis**: $f$ is linear, $H_0: f(x) = \\beta_0 + \\beta_1 x$\n",
    "\n",
    "**Test statistic**:\n",
    "\n",
    "$$F = \\frac{(\\text{RSS}_{\\text{linear}} - \\text{RSS}_{\\text{smooth}})/(\\text{df}_{\\text{smooth}} - 2)}{\\text{RSS}_{\\text{smooth}}/(n - \\text{df}_{\\text{smooth}})}$$\n",
    "\n",
    "Under $H_0$, $F \\sim F_{\\text{df}_{\\text{smooth}}-2, n-\\text{df}_{\\text{smooth}}}$ (approximately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.sort(np.random.uniform(0, 10, n))\n",
    "f_true = lambda x: np.sin(x) + 0.3 * x\n",
    "sigma = 0.4\n",
    "y = f_true(x) + np.random.normal(0, sigma, n)\n",
    "\n",
    "model = PenalizedSpline(n_knots=15, degree=3, lambda_=1.0, diff_order=2)\n",
    "model.fit(x, y)\n",
    "\n",
    "x_grid = np.linspace(0, 10, 200)\n",
    "B_grid = bspline_basis(x_grid, model.knots_, degree=3)\n",
    "f_hat = B_grid @ model.coefficients_\n",
    "\n",
    "residuals = y - model.predict(x)\n",
    "edf = model.effective_df()\n",
    "sigma_hat = np.sqrt(np.sum(residuals**2) / (n - edf))\n",
    "\n",
    "B = bspline_basis(x, model.knots_, degree=3)\n",
    "P = model._build_penalty_matrix(len(model.knots_) + 3 + 1)\n",
    "BtB_inv = np.linalg.inv(B.T @ B + model.lambda_ * P)\n",
    "cov_beta = sigma_hat**2 * BtB_inv @ (B.T @ B) @ BtB_inv\n",
    "\n",
    "se = np.sqrt(np.sum((B_grid @ cov_beta) * B_grid, axis=1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ax.plot(x_grid, f_true(x_grid), 'k--', linewidth=2.5, label='True function', alpha=0.7)\n",
    "ax.plot(x_grid, f_hat, linewidth=3, color='darkblue', label='P-spline estimate')\n",
    "ax.fill_between(x_grid, f_hat - 1.96*se, f_hat + 1.96*se, \n",
    "                alpha=0.3, color='steelblue', label='95% Confidence Band')\n",
    "ax.scatter(x, y, alpha=0.5, s=30, color='gray', label='Data', zorder=1)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "ax.set_title('P-Spline with Pointwise Confidence Bands', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Inference for P-Splines:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Estimated σ̂ = {sigma_hat:.3f} (True σ = {sigma})\")\n",
    "print(f\"Effective DF = {edf:.2f}\")\n",
    "print(f\"Mean SE = {np.mean(se):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary: Key Theoretical Results\n",
    "\n",
    "### 7.1 Optimality of Smoothing Splines\n",
    "\n",
    "**Theorem (Reinsch, 1967)**: The minimizer of\n",
    "\n",
    "$$\\sum_{i=1}^{n} (y_i - f(x_i))^2 + \\lambda \\int [f''(x)]^2 dx$$\n",
    "\n",
    "over all twice-differentiable functions is a **natural cubic spline** with knots at the unique $x_i$ values.\n",
    "\n",
    "### 7.2 Convergence of Backfitting\n",
    "\n",
    "**Theorem (Buja et al., 1989)**: The backfitting algorithm for GAMs:\n",
    "1. Converges to a unique solution when all smoothers are linear\n",
    "2. Convergence rate is geometric with constant $< 1$\n",
    "3. Solution minimizes the penalized least squares criterion\n",
    "\n",
    "### 7.3 Asymptotic Properties\n",
    "\n",
    "Under regularity conditions:\n",
    "\n",
    "$$\\|\\hat{f} - f\\|^2 = O_p(n^{-4/5})$$\n",
    "\n",
    "for smoothing splines with optimal $\\lambda \\sim n^{-4/5}$.\n",
    "\n",
    "For P-splines with fixed number of knots $K$:\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\beta} - \\beta) \\xrightarrow{d} N(0, \\Sigma)$$\n",
    "\n",
    "where $\\Sigma$ depends on $\\lambda$ and the design.\n",
    "\n",
    "### 7.4 Practical Guidelines\n",
    "\n",
    "1. **Knot Placement**: \n",
    "   - Uniform spacing sufficient for P-splines\n",
    "   - 15-30 knots typically adequate\n",
    "   \n",
    "2. **Penalty Order**:\n",
    "   - Second-order (curvature) most common\n",
    "   - Higher orders for smoother functions\n",
    "   \n",
    "3. **Smoothing Selection**:\n",
    "   - GCV for automatic selection\n",
    "   - Cross-validation for finite-sample accuracy\n",
    "   - Domain knowledge for interpretability\n",
    "   \n",
    "4. **Model Diagnostics**:\n",
    "   - Residual plots for adequacy\n",
    "   - Effective DF for complexity\n",
    "   - Confidence bands for uncertainty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}