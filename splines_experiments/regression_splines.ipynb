{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Splines\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "Given samples $(x_i, y_i)$ for $i=1,\\dots,n$, we estimate the regression function $r(x) = E(Y|X=x)$ by fitting a $k$-th order spline with knots at prespecified locations $t_1, \\dots, t_m$.\n",
    "\n",
    "We minimize:\n",
    "$$\\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^{m+k+1} \\beta_j g_j(x_i)\\right)^2 = \\|y - G\\beta\\|_2^2$$\n",
    "\n",
    "Solution: $\\hat{\\beta} = (G^T G)^{-1} G^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from splines import RegressionSpline\n",
    "from utils import (\n",
    "    generate_sinusoidal_data, \n",
    "    generate_polynomial_data,\n",
    "    plot_spline_fit, \n",
    "    mean_squared_error, \n",
    "    r_squared\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fit Cubic Regression Spline to Sinusoidal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noisy sinusoidal data\n",
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.2, \n",
    "                                            x_range=(0, 10), frequency=1.0)\n",
    "\n",
    "# Choose knot locations (5 interior knots)\n",
    "knots = np.array([2.0, 4.0, 5.0, 6.0, 8.0])\n",
    "\n",
    "# Fit cubic regression spline (degree=3)\n",
    "model = RegressionSpline(degree=3)\n",
    "model.fit(x_train, y_train, knots)\n",
    "\n",
    "# Predict on fine grid for smooth curve\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# True function\n",
    "y_true_func = lambda x: np.sin(2 * np.pi * x / 10)\n",
    "\n",
    "# Plot\n",
    "fig = plot_spline_fit(x_train, y_train, x_test, y_pred, knots, \n",
    "                      y_true_func=y_true_func,\n",
    "                      title=\"Cubic Regression Spline: Sinusoidal Data\")\n",
    "plt.show()\n",
    "\n",
    "# Compute metrics\n",
    "y_pred_train = model.predict(x_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r_squared(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Training MSE: {mse:.4f}\")\n",
    "print(f\"Training R²: {r2:.4f}\")\n",
    "print(f\"Number of knots: {len(knots)}\")\n",
    "print(f\"Number of parameters: {len(knots) + 3 + 1} = {len(model.coefficients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Effect of Number of Knots\n",
    "\n",
    "More knots → more flexibility → better fit (but risk of overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.2, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "# Try different numbers of knots\n",
    "knot_configs = [\n",
    "    (np.array([5.0]), \"1 knot\"),\n",
    "    (np.linspace(2, 8, 3), \"3 knots\"),\n",
    "    (np.linspace(2, 8, 5), \"5 knots\"),\n",
    "    (np.linspace(1, 9, 10), \"10 knots\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (knots, label) in zip(axes, knot_configs):\n",
    "    # Fit model\n",
    "    model = RegressionSpline(degree=3)\n",
    "    model.fit(x_train, y_train, knots)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Compute metrics\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, y_pred_train)\n",
    "    r2 = r_squared(y_train, y_pred_train)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x_train, y_train, alpha=0.5, s=30, label='Data', color='gray')\n",
    "    ax.plot(x_test, y_pred, 'b-', linewidth=2, label='Spline')\n",
    "    ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g--', alpha=0.5, label='True')\n",
    "    \n",
    "    for knot in knots:\n",
    "        ax.axvline(knot, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax.set_title(f'{label}\\nMSE={mse:.4f}, R²={r2:.4f}', fontsize=11)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effect of Polynomial Degree\n",
    "\n",
    "Compare linear (degree=1), quadratic (degree=2), and cubic (degree=3) splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "x_train, y_train = generate_polynomial_data(n_samples=50, degree=3, noise_std=0.3)\n",
    "x_test = np.linspace(-1, 1, 500)\n",
    "\n",
    "# Fixed knots\n",
    "knots = np.linspace(-0.6, 0.6, 4)\n",
    "\n",
    "# Try different degrees\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    model = RegressionSpline(degree=degree)\n",
    "    model.fit(x_train, y_train, knots)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Metrics\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    mse = mean_squared_error(y_train, y_pred_train)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(x_train, y_train, alpha=0.5, s=30, color='gray')\n",
    "    ax.plot(x_test, y_pred, 'b-', linewidth=2)\n",
    "    \n",
    "    for knot in knots:\n",
    "        ax.axvline(knot, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    degree_names = {1: 'Linear', 2: 'Quadratic', 3: 'Cubic'}\n",
    "    ax.set_title(f'{degree_names[degree]} Spline (k={degree})\\nMSE={mse:.4f}', fontsize=12)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Boundary Behavior Problem\n",
    "\n",
    "One problem with regression splines is that the estimates tend to display erratic behavior, i.e., they have high variance, at the boundaries.\n",
    "\n",
    "Let's demonstrate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with sparse points at boundaries\n",
    "np.random.seed(42)\n",
    "x_train = np.concatenate([\n",
    "    np.linspace(0, 1, 5),      # Few points at left boundary\n",
    "    np.linspace(1.5, 8.5, 40), # Many points in middle\n",
    "    np.linspace(9, 10, 5)      # Few points at right boundary\n",
    "])\n",
    "y_train = np.sin(2*np.pi*x_train/10) + np.random.normal(0, 0.2, len(x_train))\n",
    "\n",
    "# Fit with many knots (including near boundaries)\n",
    "knots = np.linspace(2, 8, 8)\n",
    "\n",
    "model = RegressionSpline(degree=3)\n",
    "model.fit(x_train, y_train, knots)\n",
    "\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.6, s=40, label='Training data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_pred, 'b-', linewidth=2, label='Regression spline')\n",
    "ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "\n",
    "for knot in knots:\n",
    "    ax.axvline(knot, color='r', linestyle='--', alpha=0.2)\n",
    "\n",
    "# Highlight boundary regions\n",
    "ax.axvspan(0, 1.5, alpha=0.1, color='red', label='Boundary regions')\n",
    "ax.axvspan(8.5, 10, alpha=0.1, color='red')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Boundary Variance Problem in Regression Splines', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The spline may exhibit erratic behavior (high variance) at the boundaries!\")\n",
    "print(\"This motivates the use of NATURAL SPLINES (next notebook).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with Polynomial Regression\n",
    "\n",
    "Splines are more flexible than global polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(123)\n",
    "x_train, y_train = generate_sinusoidal_data(n_samples=50, noise_std=0.25, x_range=(0, 10))\n",
    "x_test = np.linspace(0, 10, 500)\n",
    "\n",
    "# Fit regression spline\n",
    "knots = np.linspace(2, 8, 5)\n",
    "spline_model = RegressionSpline(degree=3)\n",
    "spline_model.fit(x_train, y_train, knots)\n",
    "y_spline = spline_model.predict(x_test)\n",
    "\n",
    "# Fit global polynomial (degree 7 to match number of parameters)\n",
    "n_params_spline = len(knots) + 3 + 1  # m + k + 1\n",
    "poly_degree = n_params_spline - 1\n",
    "poly_coeffs = np.polyfit(x_train, y_train, poly_degree)\n",
    "y_poly = np.polyval(poly_coeffs, x_test)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.scatter(x_train, y_train, alpha=0.5, s=30, label='Data', color='gray', zorder=3)\n",
    "ax.plot(x_test, y_spline, 'b-', linewidth=2, label=f'Regression spline ({n_params_spline} params)')\n",
    "ax.plot(x_test, y_poly, 'r-', linewidth=2, label=f'Global polynomial (degree {poly_degree})', alpha=0.7)\n",
    "ax.plot(x_test, np.sin(2*np.pi*x_test/10), 'g--', alpha=0.5, linewidth=2, label='True function')\n",
    "\n",
    "for knot in knots:\n",
    "    ax.axvline(knot, color='b', linestyle='--', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Regression Spline vs Global Polynomial', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.show()\n",
    "\n",
    "# Compare errors\n",
    "mse_spline = mean_squared_error(y_train, spline_model.predict(x_train))\n",
    "mse_poly = mean_squared_error(y_train, np.polyval(poly_coeffs, x_train))\n",
    "\n",
    "print(f\"\\nTraining MSE:\")\n",
    "print(f\"  Spline: {mse_spline:.4f}\")\n",
    "print(f\"  Polynomial: {mse_poly:.4f}\")\n",
    "print(f\"\\nSplines provide LOCAL flexibility, polynomials are GLOBAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Regression splines** fit splines to data via least squares: $\\hat{\\beta} = (G^T G)^{-1} G^T y$\n",
    "2. More knots → more flexibility → better training fit (but potential overfitting)\n",
    "3. Higher polynomial degree → smoother derivatives but more parameters\n",
    "4. **Main limitation**: High variance at boundaries (solved by natural splines)\n",
    "5. **Splines vs polynomials**: Splines offer local control, polynomials are global"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
