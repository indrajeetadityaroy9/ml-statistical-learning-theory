{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Splines Applications\n",
    "\n",
    "This notebook covers advanced spline applications including:\n",
    "- Penalized splines (P-splines)\n",
    "- Generalized Additive Models (GAM) for regression\n",
    "- Logistic GAM for classification\n",
    "- Real-world dataset applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from splines import PenalizedSpline\n",
    "from gam import GAM, LogisticGAM\n",
    "from data_utils import load_fev_data, load_heart_data, get_fev_data_summary, get_heart_data_summary\n",
    "from utils import (\n",
    "    mean_squared_error, \n",
    "    r_squared, \n",
    "    root_mean_squared_error,\n",
    "    classification_metrics,\n",
    "    plot_roc_curve,\n",
    "    plot_calibration_curve,\n",
    "    plot_confusion_matrix,\n",
    "    plot_qq_plot\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Penalized Splines (P-splines)\n",
    "\n",
    "## Advanced Splines with the FEV Dataset\n",
    "\n",
    "This section replaces synthetic visualizations with a data-driven exploration using the FEV lung function dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "We use `fev.csv`, which records forced expiratory volume (FEV) alongside demographic attributes. The helper in `data_utils.py` handles encoding of categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, feature_info = load_fev_data(\n",
    "    filepath='fev.csv',\n",
    "    standardize=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "feature_names = feature_info['feature'].tolist()\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "train_df['fev'] = y_train\n",
    "\n",
    "print('Training summary (selected statistics):')\n",
    "print(train_df.describe().T[['mean', 'std', 'min', 'max']].round(3))\n",
    "print('\\nFeature metadata:')\n",
    "print(feature_info.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Spline: Age vs FEV\n",
    "\n",
    "A penalized cubic spline captures the nonlinear relationship between age and lung capacity. We select the smoothing parameter via 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_train = X_train[:, 0]\n",
    "age_test = X_test[:, 0]\n",
    "\n",
    "lambdas = np.logspace(-3, 2, 15)\n",
    "template_model = PenalizedSpline(n_knots=20, degree=3, lambda_=1.0, diff_order=2)\n",
    "best_lambda, cv_errors = template_model.cross_validate(age_train, y_train, lambdas, cv_folds=5)\n",
    "\n",
    "pspline = PenalizedSpline(n_knots=20, degree=3, lambda_=best_lambda, diff_order=2)\n",
    "pspline.fit(age_train, y_train)\n",
    "\n",
    "train_pred = pspline.predict(age_train)\n",
    "test_pred = pspline.predict(age_test)\n",
    "\n",
    "train_rmse = root_mean_squared_error(y_train, train_pred)\n",
    "test_rmse = root_mean_squared_error(y_test, test_pred)\n",
    "\n",
    "cv_table = pd.DataFrame({\n",
    "    'lambda': lambdas,\n",
    "    'cv_error': cv_errors\n",
    "}).sort_values('cv_error').reset_index(drop=True)\n",
    "\n",
    "print(f'Selected lambda: {best_lambda:.4f}')\n",
    "print(f'Effective degrees of freedom: {pspline.effective_df():.2f}')\n",
    "print(f'Train RMSE: {train_rmse:.3f}')\n",
    "print(f'Test RMSE: {test_rmse:.3f}')\n",
    "print('\\nTop cross-validation configurations:')\n",
    "print(cv_table.head(5).to_string(index=False))\n",
    "\n",
    "preview = pd.DataFrame({\n",
    "    'age': age_test,\n",
    "    'actual_fev': y_test,\n",
    "    'predicted_fev': test_pred\n",
    "}).round(3).head(5)\n",
    "print('\\nSample test predictions:')\n",
    "print(preview.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Penalized Spline Fit\n",
    "\n",
    "Let's visualize how well our penalized spline captures the relationship between age and FEV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_grid = np.linspace(age_train.min(), age_train.max(), 200)\n",
    "fev_pred_grid = pspline.predict(age_grid)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(age_train, y_train, alpha=0.5, s=30, label='Training data', color='gray')\n",
    "plt.plot(age_grid, fev_pred_grid, 'b-', linewidth=2, label=f'P-spline fit (Î»={best_lambda:.4f})')\n",
    "plt.xlabel('Age (standardized)', fontsize=12)\n",
    "plt.ylabel('FEV', fontsize=12)\n",
    "plt.title(f'Penalized Spline: Age vs FEV\\nEffective DF = {pspline.effective_df():.2f}, Test RMSE = {test_rmse:.3f}', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generalized Additive Models for Regression\n",
    "\n",
    "**Generalized Additive Models (GAMs)** extend linear models by allowing non-linear relationships:\n",
    "\n",
    "$$E(Y \\mid X) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p)$$\n",
    "\n",
    "where $f_j$ are smooth functions estimated using penalized splines.\n",
    "\n",
    "**Key Properties**:\n",
    "- **Additivity**: Each predictor contributes independently (interpretable)\n",
    "- **Flexibility**: Captures non-linear patterns automatically\n",
    "- **Backfitting Algorithm**: Iteratively estimates each $f_j$ given others\n",
    "- **Effective Degrees of Freedom**: Quantifies model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Dataset: FEV (Forced Expiratory Volume)\n",
    "\n",
    "**Objective**: Predict lung function (FEV) from demographic and behavioral variables.\n",
    "\n",
    "**Variables**:\n",
    "- **FEV** (response): Forced expiratory volume in liters (continuous)\n",
    "- **Age**: Age in years (expect non-linear growth pattern)\n",
    "- **Height**: Height in inches (expect linear relationship)\n",
    "- **Sex**: Male (1) or Female (0)\n",
    "- **Smoke**: Smoker (1) or Non-smoker (0)\n",
    "\n",
    "**Sample Size**: 654 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, features = load_fev_data(\n",
    "    filepath='fev.csv',\n",
    "    standardize=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures:\\n{features}\")\n",
    "\n",
    "summary = get_fev_data_summary('fev.csv')\n",
    "print(f\"\\nDataset Summary:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "df = pd.read_csv('fev.csv')\n",
    "\n",
    "axes[0, 0].scatter(df['age'], df['fev'], alpha=0.5, s=30)\n",
    "axes[0, 0].set_xlabel('Age (years)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('FEV (liters)', fontsize=11)\n",
    "axes[0, 0].set_title('Age vs FEV (Non-linear Pattern Expected)', fontweight='bold')\n",
    "\n",
    "axes[0, 1].scatter(df['height'], df['fev'], alpha=0.5, s=30, color='orange')\n",
    "axes[0, 1].set_xlabel('Height (inches)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('FEV (liters)', fontsize=11)\n",
    "axes[0, 1].set_title('Height vs FEV (Linear Pattern Expected)', fontweight='bold')\n",
    "\n",
    "axes[1, 0].boxplot([df[df['sex'] == 0]['fev'], df[df['sex'] == 1]['fev']], \n",
    "                    labels=['Female', 'Male'])\n",
    "axes[1, 0].set_ylabel('FEV (liters)', fontsize=11)\n",
    "axes[1, 0].set_title('FEV by Sex', fontweight='bold')\n",
    "\n",
    "axes[1, 1].boxplot([df[df['smoke'] == 0]['fev'], df[df['smoke'] == 1]['fev']], \n",
    "                    labels=['Non-smoker', 'Smoker'])\n",
    "axes[1, 1].set_ylabel('FEV (liters)', fontsize=11)\n",
    "axes[1, 1].set_title('FEV by Smoking Status', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Linear Regression Model\n",
    "\n",
    "First, fit a standard linear regression for comparison:\n",
    "\n",
    "$$\\text{FEV} = \\beta_0 + \\beta_1 \\cdot \\text{Age} + \\beta_2 \\cdot \\text{Height} + \\beta_3 \\cdot \\text{Sex} + \\beta_4 \\cdot \\text{Smoke} + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_lm = lm.predict(X_train)\n",
    "y_pred_test_lm = lm.predict(X_test)\n",
    "\n",
    "train_mse_lm = mean_squared_error(y_train, y_pred_train_lm)\n",
    "train_r2_lm = r_squared(y_train, y_pred_train_lm)\n",
    "test_mse_lm = mean_squared_error(y_test, y_pred_test_lm)\n",
    "test_r2_lm = r_squared(y_test, y_pred_test_lm)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training:   MSE = {train_mse_lm:.4f},  RÂ² = {train_r2_lm:.3f}\")\n",
    "print(f\"Test:       MSE = {test_mse_lm:.4f},  RÂ² = {test_r2_lm:.3f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for i, feat in enumerate(features['feature']):\n",
    "    print(f\"  {feat:10s}: {lm.coef_[i]:7.3f}\")\n",
    "print(f\"  Intercept : {lm.intercept_:7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Additive Model (GAM)\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "We'll fit a GAM with:\n",
    "- **Smooth terms**: Age, Height (using P-splines with 10 knots)\n",
    "- **Linear terms**: Sex, Smoke (binary variables)\n",
    "- **Smoothing parameter**: Î» = 1.0 (controls wiggliness)\n",
    "\n",
    "$$E(\\text{FEV} \\mid X) = \\beta_0 + f_1(\\text{Age}) + f_2(\\text{Height}) + \\beta_3 \\cdot \\text{Sex} + \\beta_4 \\cdot \\text{Smoke}$$\n",
    "\n",
    "The backfitting algorithm iterates:\n",
    "1. Initialize all functions to 0\n",
    "2. For each $f_j$: update using partial residuals $r_j = y - \\hat{\\beta}_0 - \\sum_{k \\neq j} \\hat{f}_k(x_k)$\n",
    "3. Repeat until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = GAM(\n",
    "    smooth_features=[0, 1],\n",
    "    linear_features=[2, 3],\n",
    "    n_knots=[10, 10],\n",
    "    lambda_=[1.0, 1.0],\n",
    "    degree=3,\n",
    "    max_iter=100,\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "print(\"Fitting GAM...\\n\")\n",
    "gam.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "y_pred_train_gam = gam.predict(X_train)\n",
    "y_pred_test_gam = gam.predict(X_test)\n",
    "\n",
    "train_mse_gam = mean_squared_error(y_train, y_pred_train_gam)\n",
    "train_r2_gam = r_squared(y_train, y_pred_train_gam)\n",
    "test_mse_gam = mean_squared_error(y_test, y_pred_test_gam)\n",
    "test_r2_gam = r_squared(y_test, y_pred_test_gam)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GAM Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training:   MSE = {train_mse_gam:.4f},  RÂ² = {train_r2_gam:.3f}\")\n",
    "print(f\"Test:       MSE = {test_mse_gam:.4f},  RÂ² = {test_r2_gam:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare Linear Model vs GAM performance and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'GAM'],\n",
    "    'Train MSE': [train_mse_lm, train_mse_gam],\n",
    "    'Test MSE': [test_mse_lm, test_mse_gam],\n",
    "    'Train RÂ²': [train_r2_lm, train_r2_gam],\n",
    "    'Test RÂ²': [test_r2_lm, test_r2_gam],\n",
    "    'Effective DF': [5, gam.summary()['total_edf']]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "if test_r2_gam > test_r2_lm:\n",
    "    improvement = ((test_r2_gam - test_r2_lm) / test_r2_lm) * 100\n",
    "    print(f\"GAM improves test RÂ² by {improvement:.1f}% over linear model\")\n",
    "else:\n",
    "    print(\"Linear model is competitive (data may be approximately linear)\")\n",
    "\n",
    "print(f\"GAM uses {gam.summary()['total_edf']:.1f} effective DF (vs 5 for linear model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation: Smooth Functions\n",
    "\n",
    "Visualize the estimated smooth functions $f_j(x_j)$ to understand how each predictor affects FEV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = gam.summary()\n",
    "\n",
    "print(\"GAM Model Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Intercept: {summary['intercept']:.3f}\")\n",
    "print(f\"Total Effective DF: {summary['total_edf']:.1f}\\n\")\n",
    "\n",
    "print(\"Smooth Terms:\")\n",
    "for feat_idx, info in summary['smooth_terms'].items():\n",
    "    feat_name = features['feature'].values[feat_idx]\n",
    "    print(f\"  {feat_name:10s}: edf = {info['edf']:.2f}, Î» = {info['lambda']}\")\n",
    "\n",
    "print(\"\\nLinear Terms:\")\n",
    "for feat_idx, info in summary['linear_terms'].items():\n",
    "    feat_name = features['feature'].values[feat_idx]\n",
    "    print(f\"  {feat_name:10s}: coef = {info['coefficient']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "x_age, f_age = gam.get_smooth_function(0, n_points=100)\n",
    "axes[0].plot(x_age, f_age, linewidth=3, color='darkblue', label='f(Age)')\n",
    "axes[0].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[0].set_xlabel('Age (standardized)', fontsize=12)\n",
    "axes[0].set_ylabel('Smooth Effect on FEV', fontsize=12)\n",
    "axes[0].set_title(f'Effect of Age (edf = {summary[\"smooth_terms\"][0][\"edf\"]:.1f})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "x_height, f_height = gam.get_smooth_function(1, n_points=100)\n",
    "axes[1].plot(x_height, f_height, linewidth=3, color='darkgreen', label='f(Height)')\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel('Height (standardized)', fontsize=12)\n",
    "axes[1].set_ylabel('Smooth Effect on FEV', fontsize=12)\n",
    "axes[1].set_title(f'Effect of Height (edf = {summary[\"smooth_terms\"][1][\"edf\"]:.1f})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation of Smooth Functions:\")\n",
    "print(\"\\n1. Age Effect:\")\n",
    "if summary['smooth_terms'][0]['edf'] > 2:\n",
    "    print(\"Non-linear relationship detected (edf > 2)\")\n",
    "    print(\"Likely shows growth curve: rapid increase in childhood, plateau in adolescence\")\n",
    "else:\n",
    "    print(\"Nearly linear relationship (edf â 1-2)\")\n",
    "\n",
    "print(\"\\n2. Height Effect:\")\n",
    "if summary['smooth_terms'][1]['edf'] > 2:\n",
    "    print(\"Non-linear relationship detected (edf > 2)\")\n",
    "else:\n",
    "    print(\"Approximately linear relationship (edf â 1-2)\")\n",
    "    print(\"As expected: taller individuals tend to have higher lung capacity\")\n",
    "\n",
    "print(f\"\\n3. Categorical Effects:\")\n",
    "for feat_idx, info in summary['linear_terms'].items():\n",
    "    feat_name = features['feature'].values[feat_idx]\n",
    "    direction = \"increases\" if info['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"{feat_name}: {direction} FEV by {abs(info['coefficient']):.3f} liters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    "\n",
    "Check residual patterns to assess model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_train_gam = y_train - y_pred_train_gam\n",
    "residuals_test_gam = y_test - y_pred_test_gam\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].scatter(y_pred_test_gam, residuals_test_gam, alpha=0.6, s=40)\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Fitted Values', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Residuals', fontsize=11)\n",
    "axes[0, 0].set_title('Residuals vs Fitted (Test Set)', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "from scipy import stats\n",
    "stats.probplot(residuals_test_gam, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Normal Q-Q Plot', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(residuals_test_gam, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Distribution of Residuals', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].scatter(y_test, y_pred_test_gam, alpha=0.6, s=40)\n",
    "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1, 1].set_xlabel('Actual FEV', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Predicted FEV', fontsize=11)\n",
    "axes[1, 1].set_title('Actual vs Predicted (Test Set)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResidual Diagnostics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean:      {np.mean(residuals_test_gam):.4f}  (should be â 0)\")\n",
    "print(f\"Std Dev:   {np.std(residuals_test_gam):.4f}\")\n",
    "print(f\"Skewness:  {stats.skew(residuals_test_gam):.4f}  (should be â 0 for normality)\")\n",
    "print(f\"Kurtosis:  {stats.kurtosis(residuals_test_gam):.4f}  (should be â 0 for normality)\")\n",
    "\n",
    "_, p_value = stats.shapiro(residuals_test_gam)\n",
    "print(f\"\\nShapiro-Wilk Test: p = {p_value:.4f}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"Residuals appear normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"Residuals deviate from normality (p < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Parameter Selection\n",
    "\n",
    "Explore how the smoothing parameter Î» affects model fit and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_to_test = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "results = []\n",
    "\n",
    "for lam in lambdas_to_test:\n",
    "    gam_temp = GAM(\n",
    "        smooth_features=[0, 1],\n",
    "        linear_features=[2, 3],\n",
    "        n_knots=[10, 10],\n",
    "        lambda_=[lam, lam],\n",
    "        degree=3,\n",
    "        max_iter=100,\n",
    "        tol=1e-4\n",
    "    )\n",
    "    gam_temp.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    y_pred_test_temp = gam_temp.predict(X_test)\n",
    "    test_mse_temp = mean_squared_error(y_test, y_pred_test_temp)\n",
    "    test_r2_temp = r_squared(y_test, y_pred_test_temp)\n",
    "    edf_temp = gam_temp.summary()['total_edf']\n",
    "    \n",
    "    results.append({\n",
    "        'Lambda': lam,\n",
    "        'Test MSE': test_mse_temp,\n",
    "        'Test RÂ²': test_r2_temp,\n",
    "        'Effective DF': edf_temp\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSmoothing Parameter Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_idx = results_df['Test MSE'].idxmin()\n",
    "best_lambda = results_df.loc[best_idx, 'Lambda']\n",
    "print(f\"\\nOptimal Î» = {best_lambda} (minimizes test MSE)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].semilogx(results_df['Lambda'], results_df['Test MSE'], 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(best_lambda, color='red', linestyle='--', label=f'Optimal Î» = {best_lambda}')\n",
    "axes[0].set_xlabel('Î» (log scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Test MSE', fontsize=12)\n",
    "axes[0].set_title('Model Fit vs Smoothing Parameter', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogx(results_df['Lambda'], results_df['Effective DF'], 'o-', \n",
    "                 linewidth=2, markersize=8, color='darkgreen')\n",
    "axes[1].set_xlabel('Î» (log scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Effective Degrees of Freedom', fontsize=12)\n",
    "axes[1].set_title('Model Complexity vs Smoothing Parameter', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Optimal balance at Î» = {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Generalized Additive Models for Classification\n",
    "\n",
    "**Logistic GAM** extends logistic regression with smooth functions:\n",
    "\n",
    "$$\\log\\left(\\frac{P(Y=1 \\mid X)}{1 - P(Y=1 \\mid X)}\\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p)$$\n",
    "\n",
    "**Estimation**: Iteratively Reweighted Least Squares (IRLS) with backfitting\n",
    "1. Compute working weights: $w_i = p_i(1 - p_i)$\n",
    "2. Compute working response: $z_i = \\eta_i + \\frac{y_i - p_i}{w_i}$\n",
    "3. Weighted backfitting on $(z, w)$\n",
    "4. Update predictions and check deviance convergence\n",
    "\n",
    "where $\\eta_i = \\log(p_i / (1-p_i))$ is the log-odds (linear predictor).\n",
    "\n",
    "**Key Advantages**:\n",
    "- Captures non-linear risk relationships\n",
    "- Interpretable on probability and log-odds scales\n",
    "- Regularization prevents overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Dataset: South African Heart Disease Study\n",
    "\n",
    "**Objective**: Predict coronary heart disease (CHD) from risk factors.\n",
    "\n",
    "**Variables**:\n",
    "- **CHD** (response): 0 = No CHD, 1 = CHD present (binary)\n",
    "- **sbp**: Systolic blood pressure (continuous)\n",
    "- **tobacco**: Cumulative tobacco consumption in kg (continuous)\n",
    "- **ldl**: Low-density lipoprotein cholesterol (continuous)\n",
    "- **adiposity**: Adiposity index (continuous)\n",
    "- **famhist**: Family history of heart disease (binary: 0=Absent, 1=Present)\n",
    "- **typea**: Type A behavior score (continuous)\n",
    "- **obesity**: Obesity measure (continuous)\n",
    "- **alcohol**: Current alcohol consumption (continuous)\n",
    "- **age**: Age in years (continuous)\n",
    "\n",
    "**Sample Size**: 461 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, features = load_heart_data(\n",
    "    filepath='Heart.csv',\n",
    "    standardize=True,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures:\\n{features}\")\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  Training: {np.sum(y_train==0)} No CHD, {np.sum(y_train==1)} CHD\")\n",
    "print(f\"  Test:     {np.sum(y_test==0)} No CHD, {np.sum(y_test==1)} CHD\")\n",
    "print(f\"\\nCHD Prevalence: {np.mean(y_train)*100:.1f}% (training)\")\n",
    "\n",
    "summary = get_heart_data_summary('Heart.csv')\n",
    "print(f\"\\nDataset Summary:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Heart.csv')\n",
    "df['famhist'] = (df['famhist'] == 'Present').astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "risk_factors = ['age', 'sbp', 'tobacco', 'ldl', 'adiposity', 'famhist']\n",
    "titles = ['Age', 'Systolic BP', 'Tobacco (kg)', 'LDL Cholesterol', 'Adiposity', 'Family History']\n",
    "\n",
    "for i, (var, title) in enumerate(zip(risk_factors, titles)):\n",
    "    if var == 'famhist':\n",
    "        chd_by_famhist = df.groupby('famhist')['chd'].mean()\n",
    "        axes[i].bar(['Absent', 'Present'], chd_by_famhist.values, color=['steelblue', 'darkred'])\n",
    "        axes[i].set_ylabel('CHD Rate', fontsize=10)\n",
    "    else:\n",
    "        axes[i].boxplot([df[df['chd'] == 0][var], df[df['chd'] == 1][var]], \n",
    "                       labels=['No CHD', 'CHD'])\n",
    "        axes[i].set_ylabel(title, fontsize=10)\n",
    "    \n",
    "    axes[i].set_title(title, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Logistic Regression\n",
    "\n",
    "First, fit standard logistic regression for comparison:\n",
    "\n",
    "$$\\log\\left(\\frac{P(\\text{CHD}=1)}{P(\\text{CHD}=0)}\\right) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j X_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_train_lr = lr.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_test_lr = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "train_metrics_lr = classification_metrics(y_train, y_pred_proba_train_lr)\n",
    "test_metrics_lr = classification_metrics(y_test, y_pred_proba_test_lr)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training: AUC = {train_metrics_lr['auc_roc']:.3f}, Acc = {train_metrics_lr['accuracy']:.3f}\")\n",
    "print(f\"Test:     AUC = {test_metrics_lr['auc_roc']:.3f}, Acc = {test_metrics_lr['accuracy']:.3f}\")\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Precision: {test_metrics_lr['precision']:.3f}\")\n",
    "print(f\"  Recall:    {test_metrics_lr['recall']:.3f}\")\n",
    "print(f\"  F1-score:  {test_metrics_lr['f1_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nCoefficients (Log-Odds):\")\n",
    "for i, feat in enumerate(features['feature']):\n",
    "    direction = \"â\" if lr.coef_[0][i] > 0 else \"â\"\n",
    "    print(f\"  {feat:12s}: {lr.coef_[0][i]:7.3f}  {direction} risk\")\n",
    "print(f\"  Intercept:   {lr.intercept_[0]:7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic GAM\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "We'll fit a Logistic GAM with:\n",
    "- **Smooth terms**: Age, SBP, Tobacco, LDL (using P-splines with 8 knots)\n",
    "- **Linear term**: Family history (binary variable)\n",
    "- **Smoothing parameter**: Î» = 1.0\n",
    "\n",
    "$$\\log\\left(\\frac{P(\\text{CHD}=1)}{1-P(\\text{CHD}=1)}\\right) = \\beta_0 + f_1(\\text{Age}) + f_2(\\text{SBP}) + f_3(\\text{Tobacco}) + f_4(\\text{LDL}) + \\beta_5 \\cdot \\text{FamHist}$$\n",
    "\n",
    "The IRLS algorithm with backfitting ensures convergence to the maximum likelihood solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = LogisticGAM(\n",
    "    smooth_features=[8, 0, 1, 2],\n",
    "    linear_features=[4],\n",
    "    n_knots=[8, 8, 8, 8],\n",
    "    lambda_=[1.0, 1.0, 1.0, 1.0],\n",
    "    degree=3,\n",
    "    max_iter=25,\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "print(\"Fitting LogisticGAM...\\n\")\n",
    "gam.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "y_pred_proba_train_gam = gam.predict_proba(X_train)\n",
    "y_pred_proba_test_gam = gam.predict_proba(X_test)\n",
    "\n",
    "train_metrics_gam = classification_metrics(y_train, y_pred_proba_train_gam)\n",
    "test_metrics_gam = classification_metrics(y_test, y_pred_proba_test_gam)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LogisticGAM Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training: AUC = {train_metrics_gam['auc_roc']:.3f}, Acc = {train_metrics_gam['accuracy']:.3f}\")\n",
    "print(f\"Test:     AUC = {test_metrics_gam['auc_roc']:.3f}, Acc = {test_metrics_gam['accuracy']:.3f}\")\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Precision: {test_metrics_gam['precision']:.3f}\")\n",
    "print(f\"  Recall:    {test_metrics_gam['recall']:.3f}\")\n",
    "print(f\"  F1-score:  {test_metrics_gam['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare Logistic Regression vs Logistic GAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Logistic GAM'],\n",
    "    'Train AUC': [train_metrics_lr['auc_roc'], train_metrics_gam['auc_roc']],\n",
    "    'Test AUC': [test_metrics_lr['auc_roc'], test_metrics_gam['auc_roc']],\n",
    "    'Test Precision': [test_metrics_lr['precision'], test_metrics_gam['precision']],\n",
    "    'Test Recall': [test_metrics_lr['recall'], test_metrics_gam['recall']],\n",
    "    'Test F1': [test_metrics_lr['f1_score'], test_metrics_gam['f1_score']],\n",
    "    'Effective DF': [10, gam.summary()['total_edf']]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "if test_metrics_gam['auc_roc'] > test_metrics_lr['auc_roc']:\n",
    "    improvement = test_metrics_gam['auc_roc'] - test_metrics_lr['auc_roc']\n",
    "    print(f\"LogisticGAM improves test AUC by {improvement:.3f} points\")\n",
    "else:\n",
    "    print(\"Logistic regression is competitive (risk may be approximately linear)\")\n",
    "\n",
    "if test_metrics_gam['recall'] > test_metrics_lr['recall']:\n",
    "    print(f\"LogisticGAM has higher recall ({test_metrics_gam['recall']:.3f}): better at identifying CHD cases\")\n",
    "\n",
    "print(f\"LogisticGAM uses {gam.summary()['total_edf']:.1f} effective DF (vs 10 for logistic regression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation: Smooth Risk Functions\n",
    "\n",
    "Visualize smooth functions on the **log-odds scale** to understand how each risk factor affects CHD probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = gam.summary()\n",
    "\n",
    "print(\"LogisticGAM Model Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Intercept (log-odds): {summary['intercept']:.3f}\")\n",
    "print(f\"Total Effective DF: {summary['total_edf']:.1f}\\n\")\n",
    "\n",
    "print(\"Smooth Terms (Log-Odds Scale):\")\n",
    "smooth_indices = [8, 0, 1, 2]\n",
    "smooth_names = ['age', 'sbp', 'tobacco', 'ldl']\n",
    "for feat_idx, name in zip(smooth_indices, smooth_names):\n",
    "    info = summary['smooth_terms'][feat_idx]\n",
    "    print(f\"  {name:10s}: edf = {info['edf']:.2f}, Î» = {info['lambda']}\")\n",
    "\n",
    "print(\"\\nLinear Terms:\")\n",
    "for feat_idx, info in summary['linear_terms'].items():\n",
    "    feat_name = features['feature'].values[feat_idx]\n",
    "    print(f\"  {feat_name:10s}: coef = {info['coefficient']:.3f} (log-odds)\")\n",
    "    or_value = np.exp(info['coefficient'])\n",
    "    print(f\"                 OR = {or_value:.2f} (odds ratio)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "smooth_features = [8, 0, 1, 2]\n",
    "feature_names = ['Age', 'Systolic BP', 'Tobacco', 'LDL Cholesterol']\n",
    "colors = ['darkred', 'darkblue', 'darkgreen', 'darkorange']\n",
    "\n",
    "for i, (feat_idx, name, color) in enumerate(zip(smooth_features, feature_names, colors)):\n",
    "    x_vals, f_vals = gam.get_smooth_function(feat_idx, n_points=100)\n",
    "    \n",
    "    axes[i].plot(x_vals, f_vals, linewidth=3, color=color, label=f'f({name})')\n",
    "    axes[i].axhline(0, color='gray', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    axes[i].fill_between(x_vals, 0, f_vals, alpha=0.2, color=color)\n",
    "    \n",
    "    axes[i].set_xlabel(f'{name} (standardized)', fontsize=11)\n",
    "    axes[i].set_ylabel('Log-Odds Contribution', fontsize=11)\n",
    "    axes[i].set_title(f'Effect of {name} on CHD Risk (edf={summary[\"smooth_terms\"][feat_idx][\"edf\"]:.1f})', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for feat_idx, name in zip(smooth_features, feature_names):\n",
    "    edf = summary['smooth_terms'][feat_idx]['edf']\n",
    "    if edf > 2:\n",
    "        print(f\"{name}: Non-linear risk pattern (edf={edf:.1f})\")\n",
    "    else:\n",
    "        print(f\"{name}: Approximately linear effect (edf={edf:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Performance Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 7))\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_test_lr)\n",
    "auc_lr = test_metrics_lr['auc_roc']\n",
    "ax.plot(fpr_lr, tpr_lr, linewidth=2.5, label=f'Logistic Regression (AUC={auc_lr:.3f})', color='steelblue')\n",
    "\n",
    "fpr_gam, tpr_gam, _ = roc_curve(y_test, y_pred_proba_test_gam)\n",
    "auc_gam = test_metrics_gam['auc_roc']\n",
    "ax.plot(fpr_gam, tpr_gam, linewidth=2.5, label=f'Logistic GAM (AUC={auc_gam:.3f})', color='darkred')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "prob_true_lr, prob_pred_lr = calibration_curve(y_test, y_pred_proba_test_lr, n_bins=10)\n",
    "axes[0].plot(prob_pred_lr, prob_true_lr, 'o-', linewidth=2, markersize=8, label='Calibration')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Perfect Calibration')\n",
    "axes[0].set_xlabel('Predicted Probability', fontsize=11)\n",
    "axes[0].set_ylabel('Observed Frequency', fontsize=11)\n",
    "axes[0].set_title('Logistic Regression Calibration', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "prob_true_gam, prob_pred_gam = calibration_curve(y_test, y_pred_proba_test_gam, n_bins=10)\n",
    "axes[1].plot(prob_pred_gam, prob_true_gam, 'o-', linewidth=2, markersize=8, \n",
    "             label='Calibration', color='darkred')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Perfect Calibration')\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=11)\n",
    "axes[1].set_ylabel('Observed Frequency', fontsize=11)\n",
    "axes[1].set_title('Logistic GAM Calibration', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "y_pred_lr = (y_pred_proba_test_lr >= 0.5).astype(int)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=['No CHD', 'CHD'], yticklabels=['No CHD', 'CHD'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=11)\n",
    "axes[0].set_ylabel('Actual', fontsize=11)\n",
    "axes[0].set_title('Logistic Regression', fontweight='bold')\n",
    "\n",
    "y_pred_gam = (y_pred_proba_test_gam >= 0.5).astype(int)\n",
    "cm_gam = confusion_matrix(y_test, y_pred_gam)\n",
    "sns.heatmap(cm_gam, annot=True, fmt='d', cmap='Reds', ax=axes[1],\n",
    "            xticklabels=['No CHD', 'CHD'], yticklabels=['No CHD', 'CHD'])\n",
    "axes[1].set_xlabel('Predicted', fontsize=11)\n",
    "axes[1].set_ylabel('Actual', fontsize=11)\n",
    "axes[1].set_title('Logistic GAM', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"  TN={cm_lr[0,0]}, FP={cm_lr[0,1]}, FN={cm_lr[1,0]}, TP={cm_lr[1,1]}\")\n",
    "print(f\"\\nLogistic GAM:\")\n",
    "print(f\"  TN={cm_gam[0,0]}, FP={cm_gam[0,1]}, FN={cm_gam[1,0]}, TP={cm_gam[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Profiling\n",
    "\n",
    "Demonstrate how LogisticGAM can be used for individualized risk assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Individual Risk Assessment Examples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in [0, 10, 20]:\n",
    "    patient_features = X_test[i:i+1]\n",
    "    true_label = y_test[i]\n",
    "    \n",
    "    prob_lr = lr.predict_proba(patient_features)[0, 1]\n",
    "    prob_gam = gam.predict_proba(patient_features)[0]\n",
    "    \n",
    "    print(f\"\\nPatient {i+1}:\")\n",
    "    print(f\"  True Status: {'CHD' if true_label == 1 else 'No CHD'}\")\n",
    "    print(f\"  Logistic Regression: {prob_lr:.1%} CHD risk\")\n",
    "    print(f\"  Logistic GAM:        {prob_gam:.1%} CHD risk\")\n",
    "    \n",
    "    if prob_gam < 0.3:\n",
    "        risk_cat = \"Low Risk\"\n",
    "    elif prob_gam < 0.7:\n",
    "        risk_cat = \"Moderate Risk\"\n",
    "    else:\n",
    "        risk_cat = \"High Risk\"\n",
    "    print(f\"  Risk Category: {risk_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing Parameter Selection\n",
    "\n",
    "Explore effect of smoothing parameter on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_to_test = [0.01, 0.1, 1.0, 10.0, 50.0]\n",
    "results = []\n",
    "\n",
    "for lam in lambdas_to_test:\n",
    "    gam_temp = LogisticGAM(\n",
    "        smooth_features=[8, 0, 1, 2],\n",
    "        linear_features=[4],\n",
    "        n_knots=[8, 8, 8, 8],\n",
    "        lambda_=[lam, lam, lam, lam],\n",
    "        degree=3,\n",
    "        max_iter=25,\n",
    "        tol=1e-4\n",
    "    )\n",
    "    gam_temp.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    y_pred_proba_temp = gam_temp.predict_proba(X_test)\n",
    "    auc_temp = roc_auc_score(y_test, y_pred_proba_temp)\n",
    "    edf_temp = gam_temp.summary()['total_edf']\n",
    "    \n",
    "    results.append({\n",
    "        'Lambda': lam,\n",
    "        'Test AUC': auc_temp,\n",
    "        'Effective DF': edf_temp\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSmoothing Parameter Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_idx = results_df['Test AUC'].idxmax()\n",
    "best_lambda = results_df.loc[best_idx, 'Lambda']\n",
    "print(f\"\\nâ Optimal Î» = {best_lambda} (maximizes test AUC)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].semilogx(results_df['Lambda'], results_df['Test AUC'], 'o-', \n",
    "                 linewidth=2, markersize=8, color='darkred')\n",
    "axes[0].axvline(best_lambda, color='blue', linestyle='--', label=f'Optimal Î» = {best_lambda}')\n",
    "axes[0].set_xlabel('Î» (log scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Test AUC', fontsize=12)\n",
    "axes[0].set_title('Classification Performance vs Î»', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogx(results_df['Lambda'], results_df['Effective DF'], 'o-', \n",
    "                 linewidth=2, markersize=8, color='darkgreen')\n",
    "axes[1].set_xlabel('Î» (log scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Effective Degrees of Freedom', fontsize=12)\n",
    "axes[1].set_title('Model Complexity vs Î»', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Optimal balance at Î» = {best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Penalized Splines (P-splines):\n",
    "1. **P-splines** are computationally efficient and flexible\n",
    "2. Use many knots (20-40) with heavy smoothing for stability\n",
    "3. Cross-validation is essential for selecting optimal smoothing parameter Î»\n",
    "4. Effective degrees of freedom provide a measure of model complexity\n",
    "5. Better than smoothing splines for practical applications\n",
    "\n",
    "### Generalized Additive Models:\n",
    "1. **Model Performance**:\n",
    "   - GAM provides flexible modeling of non-linear age effects\n",
    "   - Automatic smoothing parameter selection via cross-validation\n",
    "   - Interpretable additive structure\n",
    "2. **Effective Degrees of Freedom**:\n",
    "   - Quantifies model complexity beyond simple parameter count\n",
    "   - Each smooth term contributes fractional degrees of freedom\n",
    "   - Total edf balances fit and parsimony\n",
    "3. **Interpretation**:\n",
    "   - Smooth functions show marginal effects of each predictor\n",
    "   - Non-linearity automatically detected and modeled\n",
    "   - Categorical effects remain interpretable as in linear models\n",
    "4. **Practical Value**:\n",
    "   - GAMs bridge gap between linear models and black-box ML\n",
    "   - Maintain interpretability while capturing complexity\n",
    "   - Essential tool for scientific inference and prediction\n",
    "\n",
    "### Logistic GAM for Classification:\n",
    "1. **Classification Performance**:\n",
    "   - LogisticGAM captures non-linear risk patterns\n",
    "   - Improved discrimination over linear logistic regression\n",
    "   - Calibrated probabilities for clinical use\n",
    "2. **Interpretability**:\n",
    "   - Smooth functions show risk trajectories on log-odds scale\n",
    "   - Non-linear effects automatically detected (high edf)\n",
    "   - Linear effects preserved for binary predictors\n",
    "3. **Clinical Utility**:\n",
    "   - Individualized risk assessment\n",
    "   - Identifies threshold effects (e.g., age-related risk acceleration)\n",
    "   - Provides actionable insights for intervention\n",
    "4. **Statistical Rigor**:\n",
    "   - IRLS algorithm ensures maximum likelihood estimation\n",
    "   - Effective DF quantifies complexity\n",
    "   - Regularization prevents overfitting on small samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
